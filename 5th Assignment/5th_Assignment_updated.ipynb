{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5th Assignment",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kishoresmeda/EIP_internship_work/blob/master/5th%20Assignment/5th_Assignment_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "f37c230a-3c0b-4eb0-d51a-eec6b681420e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Tu5IEpEZ1j",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "4c4991a8-070d-4f82-f3de-816ac372f68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "f2509676-fab9-457e-dbf2-18417b472d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBTOzYO0seQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df['image_path'] = 'hvc_data/' + df['image_path'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvLdDBGHHdby",
        "colab_type": "code",
        "outputId": "04faa43f-2f3d-4164-add1-716c06a32b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "808bd7fc-3753-4f2e-e984-58dea50964d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            images = self.augmentation.flow(images, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "899fd085-2fb7-4a3d-b53b-ddaa3e671167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "fa4cc6f4-b7b8-418e-cdfb-72616ce45384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>resized/580.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>resized/43.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12400</th>\n",
              "      <td>resized/12402.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11694</th>\n",
              "      <td>resized/11696.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13292</th>\n",
              "      <td>resized/13294.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "579      resized/580.jpg              1  ...                        1              0\n",
              "42        resized/43.jpg              0  ...                        1              0\n",
              "12400  resized/12402.jpg              0  ...                        0              1\n",
              "11694  resized/11696.jpg              1  ...                        1              0\n",
              "13292  resized/13294.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "# train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=32, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "    )\n",
        ")\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "7a6fb178-a56b-49ed-e9a0-d76d47d408db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4WgGYl_mSOQ",
        "colab_type": "code",
        "outputId": "977b07f6-d4e2-404b-df4b-7c157480a59d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6-sDJvmmVk_",
        "colab_type": "code",
        "outputId": "29046fe5-36b4-4a24-ce2e-811806fac26c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(targets)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_PK9ixLrQ_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Triangle cyclic learning rate policy\n",
        "\n",
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.1,step_size=782.)\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = '/content/gdrive/My Drive/model_at_epoch_{epoch}.hdf5'\n",
        "ckpt =ModelCheckpoint(filepath, verbose=1, monitor='val_loss', save_best_only=True,save_weights_only=False, mode='auto',period=1)\n",
        "callback = [ckpt,clr]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD0A77DRrlCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Referred from https://github.com/raghakot/keras-resnet/blob/master/resnet.py\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.common.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uKk6wWar53N",
        "colab_type": "code",
        "outputId": "936c0f91-129b-4053-ac78-4912a3981975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "backbone = ResnetBuilder.build_resnet_34((3,224,224), 8)\n",
        "\n",
        "neck = backbone.output\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET0mczj1sDCz",
        "colab_type": "code",
        "outputId": "ca737737-681d-4b89-b4bf-fe2e378a606d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "losses = {\"gender_output\": \"binary_crossentropy\",\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\"age_output\": \"categorical_crossentropy\",\n",
        "\"weight_output\": \"categorical_crossentropy\",\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.1,\n",
        "                        step_size=782.)\n",
        "from keras import optimizers\n",
        "opt = keras.optimizers.SGD(lr=0.001,momentum=1e-2)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfGdwKl8sKlt",
        "colab_type": "code",
        "outputId": "e88d280f-05ef-4c45-d966-48196c90e576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(generator=train_gen, validation_data=valid_gen, use_multiprocessing = True,\n",
        "    workers=-1, epochs=50, verbose=1, callbacks = [clr, ckpt])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "360/360 [==============================] - 87s 240ms/step - loss: 9.9393 - gender_output_loss: 0.6867 - image_quality_output_loss: 1.0033 - age_output_loss: 1.4810 - weight_output_loss: 1.0765 - bag_output_loss: 0.9600 - footwear_output_loss: 1.0571 - pose_output_loss: 0.9655 - emotion_output_loss: 1.0075 - gender_output_acc: 0.5468 - image_quality_output_acc: 0.5569 - age_output_acc: 0.3885 - weight_output_acc: 0.6079 - bag_output_acc: 0.5595 - footwear_output_acc: 0.4179 - pose_output_acc: 0.5878 - emotion_output_acc: 0.6916 - val_loss: 9.5584 - val_gender_output_loss: 0.6830 - val_image_quality_output_loss: 0.9975 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.9106 - val_footwear_output_loss: 1.0393 - val_pose_output_loss: 0.9192 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.5655 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.4561 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 9.55843, saving model to /content/gdrive/My Drive/model_at_epoch_1.hdf5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 72s 200ms/step - loss: 9.5183 - gender_output_loss: 0.6850 - image_quality_output_loss: 0.9747 - age_output_loss: 1.4304 - weight_output_loss: 0.9829 - bag_output_loss: 0.9179 - footwear_output_loss: 1.0028 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9062 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5569 - age_output_acc: 0.3956 - weight_output_acc: 0.6360 - bag_output_acc: 0.5633 - footwear_output_acc: 0.5109 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.5777 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 0.9966 - val_age_output_loss: 1.4178 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.9112 - val_footwear_output_loss: 1.0717 - val_pose_output_loss: 0.9191 - val_emotion_output_loss: 0.9072 - val_gender_output_acc: 0.5655 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 9.55843\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 72s 199ms/step - loss: 9.4349 - gender_output_loss: 0.6831 - image_quality_output_loss: 0.9579 - age_output_loss: 1.4286 - weight_output_loss: 0.9813 - bag_output_loss: 0.9152 - footwear_output_loss: 0.9676 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9037 - gender_output_acc: 0.5625 - image_quality_output_acc: 0.5561 - age_output_acc: 0.3955 - weight_output_acc: 0.6360 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5395 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.5767 - val_gender_output_loss: 0.6891 - val_image_quality_output_loss: 1.0296 - val_age_output_loss: 1.4126 - val_weight_output_loss: 0.9897 - val_bag_output_loss: 0.9089 - val_footwear_output_loss: 1.0488 - val_pose_output_loss: 0.9191 - val_emotion_output_loss: 0.9183 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.4677 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 9.55843\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 9.3691 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9345 - age_output_loss: 1.4261 - weight_output_loss: 0.9789 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9527 - pose_output_loss: 0.9261 - emotion_output_loss: 0.9017 - gender_output_acc: 0.5656 - image_quality_output_acc: 0.5565 - age_output_acc: 0.3955 - weight_output_acc: 0.6360 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5496 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.4139 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9909 - val_age_output_loss: 1.4097 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9077 - val_footwear_output_loss: 0.9596 - val_pose_output_loss: 0.9168 - val_emotion_output_loss: 0.9073 - val_gender_output_acc: 0.5605 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5650 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00004: val_loss improved from 9.55843 to 9.41393, saving model to /content/gdrive/My Drive/model_at_epoch_4.hdf5\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 71s 198ms/step - loss: 9.3279 - gender_output_loss: 0.6799 - image_quality_output_loss: 0.9239 - age_output_loss: 1.4249 - weight_output_loss: 0.9771 - bag_output_loss: 0.9117 - footwear_output_loss: 0.9348 - pose_output_loss: 0.9252 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5570 - age_output_acc: 0.3956 - weight_output_acc: 0.6360 - bag_output_acc: 0.5631 - footwear_output_acc: 0.5656 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.4265 - val_gender_output_loss: 0.6813 - val_image_quality_output_loss: 0.9896 - val_age_output_loss: 1.4096 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9069 - val_footwear_output_loss: 0.9790 - val_pose_output_loss: 0.9157 - val_emotion_output_loss: 0.9105 - val_gender_output_acc: 0.5716 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5232 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 9.41393\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 9.3388 - gender_output_loss: 0.6806 - image_quality_output_loss: 0.9256 - age_output_loss: 1.4247 - weight_output_loss: 0.9778 - bag_output_loss: 0.9105 - footwear_output_loss: 0.9508 - pose_output_loss: 0.9253 - emotion_output_loss: 0.9012 - gender_output_acc: 0.5656 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3956 - weight_output_acc: 0.6360 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5501 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.7928 - val_gender_output_loss: 0.6891 - val_image_quality_output_loss: 1.1342 - val_age_output_loss: 1.4191 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 1.1551 - val_pose_output_loss: 0.9220 - val_emotion_output_loss: 0.9339 - val_gender_output_acc: 0.5318 - val_image_quality_output_acc: 0.4567 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.4476 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 9.41393\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 9.3224 - gender_output_loss: 0.6784 - image_quality_output_loss: 0.9275 - age_output_loss: 1.4244 - weight_output_loss: 0.9783 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9499 - pose_output_loss: 0.9256 - emotion_output_loss: 0.9015 - gender_output_acc: 0.5690 - image_quality_output_acc: 0.5570 - age_output_acc: 0.3955 - weight_output_acc: 0.6360 - bag_output_acc: 0.5647 - footwear_output_acc: 0.5534 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.2766 - val_gender_output_loss: 0.6644 - val_image_quality_output_loss: 0.9464 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8977 - val_footwear_output_loss: 0.9530 - val_pose_output_loss: 0.9152 - val_emotion_output_loss: 0.9028 - val_gender_output_acc: 0.5781 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5680 - val_footwear_output_acc: 0.5383 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00007: val_loss improved from 9.41393 to 9.27661, saving model to /content/gdrive/My Drive/model_at_epoch_7.hdf5\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 9.2620 - gender_output_loss: 0.6718 - image_quality_output_loss: 0.9177 - age_output_loss: 1.4207 - weight_output_loss: 0.9766 - bag_output_loss: 0.9080 - footwear_output_loss: 0.9349 - pose_output_loss: 0.9241 - emotion_output_loss: 0.9007 - gender_output_acc: 0.5748 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3959 - weight_output_acc: 0.6360 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5648 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.2712 - val_gender_output_loss: 0.6658 - val_image_quality_output_loss: 0.9663 - val_age_output_loss: 1.3998 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8984 - val_footwear_output_loss: 0.9355 - val_pose_output_loss: 0.9129 - val_emotion_output_loss: 0.9076 - val_gender_output_acc: 0.5958 - val_image_quality_output_acc: 0.5181 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.5595 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00008: val_loss improved from 9.27661 to 9.27117, saving model to /content/gdrive/My Drive/model_at_epoch_8.hdf5\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 9.1977 - gender_output_loss: 0.6630 - image_quality_output_loss: 0.9058 - age_output_loss: 1.4156 - weight_output_loss: 0.9728 - bag_output_loss: 0.9039 - footwear_output_loss: 0.9163 - pose_output_loss: 0.9222 - emotion_output_loss: 0.8983 - gender_output_acc: 0.5887 - image_quality_output_acc: 0.5629 - age_output_acc: 0.3957 - weight_output_acc: 0.6360 - bag_output_acc: 0.5675 - footwear_output_acc: 0.5755 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.3334 - val_gender_output_loss: 0.6700 - val_image_quality_output_loss: 1.0342 - val_age_output_loss: 1.4006 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.8998 - val_footwear_output_loss: 0.9229 - val_pose_output_loss: 0.9132 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.5832 - val_image_quality_output_acc: 0.4708 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5776 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 9.27117\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 70s 196ms/step - loss: 9.1989 - gender_output_loss: 0.6609 - image_quality_output_loss: 0.9070 - age_output_loss: 1.4163 - weight_output_loss: 0.9741 - bag_output_loss: 0.9047 - footwear_output_loss: 0.9198 - pose_output_loss: 0.9223 - emotion_output_loss: 0.8985 - gender_output_acc: 0.5940 - image_quality_output_acc: 0.5612 - age_output_acc: 0.3955 - weight_output_acc: 0.6360 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5692 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.3183 - val_gender_output_loss: 0.7071 - val_image_quality_output_loss: 0.9128 - val_age_output_loss: 1.4145 - val_weight_output_loss: 0.9911 - val_bag_output_loss: 0.9053 - val_footwear_output_loss: 0.9785 - val_pose_output_loss: 0.9114 - val_emotion_output_loss: 0.9076 - val_gender_output_acc: 0.4526 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5696 - val_footwear_output_acc: 0.5242 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 9.27117\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 9.2112 - gender_output_loss: 0.6617 - image_quality_output_loss: 0.9157 - age_output_loss: 1.4183 - weight_output_loss: 0.9768 - bag_output_loss: 0.9063 - footwear_output_loss: 0.9272 - pose_output_loss: 0.9231 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5906 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3948 - weight_output_acc: 0.6360 - bag_output_acc: 0.5658 - footwear_output_acc: 0.5679 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.7063 - val_gender_output_loss: 0.7184 - val_image_quality_output_loss: 1.2371 - val_age_output_loss: 1.4187 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9103 - val_footwear_output_loss: 1.0155 - val_pose_output_loss: 0.9165 - val_emotion_output_loss: 0.9308 - val_gender_output_acc: 0.4501 - val_image_quality_output_acc: 0.2928 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.5086 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 9.27117\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 71s 198ms/step - loss: 9.1641 - gender_output_loss: 0.6527 - image_quality_output_loss: 0.9145 - age_output_loss: 1.4165 - weight_output_loss: 0.9762 - bag_output_loss: 0.9010 - footwear_output_loss: 0.9216 - pose_output_loss: 0.9194 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6037 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3953 - weight_output_acc: 0.6360 - bag_output_acc: 0.5650 - footwear_output_acc: 0.5711 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.2125 - val_gender_output_loss: 0.7020 - val_image_quality_output_loss: 0.9206 - val_age_output_loss: 1.4060 - val_weight_output_loss: 0.9848 - val_bag_output_loss: 0.9005 - val_footwear_output_loss: 0.9285 - val_pose_output_loss: 0.9107 - val_emotion_output_loss: 0.9035 - val_gender_output_acc: 0.5040 - val_image_quality_output_acc: 0.5323 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.5751 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00012: val_loss improved from 9.27117 to 9.21254, saving model to /content/gdrive/My Drive/model_at_epoch_12.hdf5\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 71s 198ms/step - loss: 9.0751 - gender_output_loss: 0.6338 - image_quality_output_loss: 0.9014 - age_output_loss: 1.4117 - weight_output_loss: 0.9727 - bag_output_loss: 0.8931 - footwear_output_loss: 0.8976 - pose_output_loss: 0.9159 - emotion_output_loss: 0.8968 - gender_output_acc: 0.6290 - image_quality_output_acc: 0.5630 - age_output_acc: 0.3949 - weight_output_acc: 0.6360 - bag_output_acc: 0.5701 - footwear_output_acc: 0.5814 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.0488 - val_gender_output_loss: 0.6159 - val_image_quality_output_loss: 0.9242 - val_age_output_loss: 1.3900 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.8838 - val_footwear_output_loss: 0.8966 - val_pose_output_loss: 0.9053 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.6487 - val_image_quality_output_acc: 0.5398 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5776 - val_footwear_output_acc: 0.5877 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00013: val_loss improved from 9.21254 to 9.04883, saving model to /content/gdrive/My Drive/model_at_epoch_13.hdf5\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 9.0387 - gender_output_loss: 0.6241 - image_quality_output_loss: 0.8960 - age_output_loss: 1.4093 - weight_output_loss: 0.9721 - bag_output_loss: 0.8907 - footwear_output_loss: 0.8888 - pose_output_loss: 0.9130 - emotion_output_loss: 0.8960 - gender_output_acc: 0.6413 - image_quality_output_acc: 0.5684 - age_output_acc: 0.3947 - weight_output_acc: 0.6360 - bag_output_acc: 0.5733 - footwear_output_acc: 0.5893 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.2924 - val_gender_output_loss: 0.6981 - val_image_quality_output_loss: 1.0096 - val_age_output_loss: 1.4082 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.8936 - val_footwear_output_loss: 0.9372 - val_pose_output_loss: 0.9100 - val_emotion_output_loss: 0.9029 - val_gender_output_acc: 0.5242 - val_image_quality_output_acc: 0.4839 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5524 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 9.04883\n",
            "Epoch 15/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 9.0854 - gender_output_loss: 0.6367 - image_quality_output_loss: 0.9110 - age_output_loss: 1.4115 - weight_output_loss: 0.9750 - bag_output_loss: 0.8975 - footwear_output_loss: 0.9031 - pose_output_loss: 0.9147 - emotion_output_loss: 0.8968 - gender_output_acc: 0.6233 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3944 - weight_output_acc: 0.6360 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5820 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.1377 - val_gender_output_loss: 0.6607 - val_image_quality_output_loss: 0.9255 - val_age_output_loss: 1.3994 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8976 - val_footwear_output_loss: 0.9389 - val_pose_output_loss: 0.8949 - val_emotion_output_loss: 0.9058 - val_gender_output_acc: 0.5842 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5620 - val_footwear_output_acc: 0.5534 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 9.04883\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 9.0683 - gender_output_loss: 0.6346 - image_quality_output_loss: 0.9149 - age_output_loss: 1.4118 - weight_output_loss: 0.9756 - bag_output_loss: 0.8965 - footwear_output_loss: 0.9035 - pose_output_loss: 0.9131 - emotion_output_loss: 0.8967 - gender_output_acc: 0.6301 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3957 - weight_output_acc: 0.6360 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5844 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 9.0788 - val_gender_output_loss: 0.6250 - val_image_quality_output_loss: 0.9875 - val_age_output_loss: 1.3917 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.8766 - val_footwear_output_loss: 0.9041 - val_pose_output_loss: 0.9001 - val_emotion_output_loss: 0.8979 - val_gender_output_acc: 0.6351 - val_image_quality_output_acc: 0.5181 - val_age_output_acc: 0.4148 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.5882 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 9.04883\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.9663 - gender_output_loss: 0.6112 - image_quality_output_loss: 0.9030 - age_output_loss: 1.4086 - weight_output_loss: 0.9727 - bag_output_loss: 0.8880 - footwear_output_loss: 0.8715 - pose_output_loss: 0.9084 - emotion_output_loss: 0.8949 - gender_output_acc: 0.6523 - image_quality_output_acc: 0.5635 - age_output_acc: 0.3947 - weight_output_acc: 0.6360 - bag_output_acc: 0.5794 - footwear_output_acc: 0.6016 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 8.8746 - val_gender_output_loss: 0.5790 - val_image_quality_output_loss: 0.9203 - val_age_output_loss: 1.3860 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8654 - val_footwear_output_loss: 0.8585 - val_pose_output_loss: 0.8861 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.6865 - val_image_quality_output_acc: 0.5383 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.6074 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00017: val_loss improved from 9.04883 to 8.87463, saving model to /content/gdrive/My Drive/model_at_epoch_17.hdf5\n",
            "Epoch 18/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.8994 - gender_output_loss: 0.5956 - image_quality_output_loss: 0.8921 - age_output_loss: 1.4058 - weight_output_loss: 0.9708 - bag_output_loss: 0.8814 - footwear_output_loss: 0.8544 - pose_output_loss: 0.9032 - emotion_output_loss: 0.8926 - gender_output_acc: 0.6692 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3954 - weight_output_acc: 0.6360 - bag_output_acc: 0.5815 - footwear_output_acc: 0.6068 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7123 - val_loss: 8.9822 - val_gender_output_loss: 0.6417 - val_image_quality_output_loss: 0.9405 - val_age_output_loss: 1.3950 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8776 - val_footwear_output_loss: 0.8624 - val_pose_output_loss: 0.8848 - val_emotion_output_loss: 0.8965 - val_gender_output_acc: 0.6326 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 8.87463\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.9424 - gender_output_loss: 0.6015 - image_quality_output_loss: 0.9061 - age_output_loss: 1.4057 - weight_output_loss: 0.9725 - bag_output_loss: 0.8877 - footwear_output_loss: 0.8712 - pose_output_loss: 0.9060 - emotion_output_loss: 0.8946 - gender_output_acc: 0.6618 - image_quality_output_acc: 0.5592 - age_output_acc: 0.3962 - weight_output_acc: 0.6360 - bag_output_acc: 0.5787 - footwear_output_acc: 0.6009 - pose_output_acc: 0.6176 - emotion_output_acc: 0.7123 - val_loss: 9.5919 - val_gender_output_loss: 0.8330 - val_image_quality_output_loss: 1.1105 - val_age_output_loss: 1.4166 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.9212 - val_footwear_output_loss: 0.9999 - val_pose_output_loss: 0.9214 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.4693 - val_image_quality_output_acc: 0.4148 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.4889 - val_footwear_output_acc: 0.5257 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 8.87463\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.9724 - gender_output_loss: 0.6115 - image_quality_output_loss: 0.9133 - age_output_loss: 1.4118 - weight_output_loss: 0.9746 - bag_output_loss: 0.8916 - footwear_output_loss: 0.8855 - pose_output_loss: 0.9061 - emotion_output_loss: 0.8959 - gender_output_acc: 0.6541 - image_quality_output_acc: 0.5599 - age_output_acc: 0.3935 - weight_output_acc: 0.6360 - bag_output_acc: 0.5761 - footwear_output_acc: 0.5931 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7123 - val_loss: 9.5454 - val_gender_output_loss: 0.8500 - val_image_quality_output_loss: 1.0782 - val_age_output_loss: 1.4165 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.9276 - val_footwear_output_loss: 0.9907 - val_pose_output_loss: 0.9106 - val_emotion_output_loss: 0.9119 - val_gender_output_acc: 0.4718 - val_image_quality_output_acc: 0.4713 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.4612 - val_footwear_output_acc: 0.4743 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 8.87463\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 71s 198ms/step - loss: 8.8750 - gender_output_loss: 0.5855 - image_quality_output_loss: 0.9043 - age_output_loss: 1.4076 - weight_output_loss: 0.9716 - bag_output_loss: 0.8838 - footwear_output_loss: 0.8659 - pose_output_loss: 0.8957 - emotion_output_loss: 0.8940 - gender_output_acc: 0.6777 - image_quality_output_acc: 0.5645 - age_output_acc: 0.3950 - weight_output_acc: 0.6360 - bag_output_acc: 0.5833 - footwear_output_acc: 0.6063 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7123 - val_loss: 8.7610 - val_gender_output_loss: 0.5488 - val_image_quality_output_loss: 0.9075 - val_age_output_loss: 1.3833 - val_weight_output_loss: 0.9811 - val_bag_output_loss: 0.8665 - val_footwear_output_loss: 0.8424 - val_pose_output_loss: 0.8763 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7152 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5978 - val_footwear_output_acc: 0.6134 - val_pose_output_acc: 0.6210 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00021: val_loss improved from 8.87463 to 8.76097, saving model to /content/gdrive/My Drive/model_at_epoch_21.hdf5\n",
            "Epoch 22/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 8.7754 - gender_output_loss: 0.5541 - image_quality_output_loss: 0.8937 - age_output_loss: 1.4031 - weight_output_loss: 0.9687 - bag_output_loss: 0.8774 - footwear_output_loss: 0.8377 - pose_output_loss: 0.8897 - emotion_output_loss: 0.8913 - gender_output_acc: 0.7110 - image_quality_output_acc: 0.5652 - age_output_acc: 0.3963 - weight_output_acc: 0.6360 - bag_output_acc: 0.5941 - footwear_output_acc: 0.6203 - pose_output_acc: 0.6210 - emotion_output_acc: 0.7123 - val_loss: 8.7348 - val_gender_output_loss: 0.5313 - val_image_quality_output_loss: 0.9173 - val_age_output_loss: 1.3830 - val_weight_output_loss: 0.9807 - val_bag_output_loss: 0.8613 - val_footwear_output_loss: 0.8377 - val_pose_output_loss: 0.8731 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.7374 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.6245 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00022: val_loss improved from 8.76097 to 8.73483, saving model to /content/gdrive/My Drive/model_at_epoch_22.hdf5\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 8.7853 - gender_output_loss: 0.5575 - image_quality_output_loss: 0.8974 - age_output_loss: 1.4042 - weight_output_loss: 0.9711 - bag_output_loss: 0.8761 - footwear_output_loss: 0.8449 - pose_output_loss: 0.8867 - emotion_output_loss: 0.8919 - gender_output_acc: 0.7057 - image_quality_output_acc: 0.5680 - age_output_acc: 0.3962 - weight_output_acc: 0.6360 - bag_output_acc: 0.5895 - footwear_output_acc: 0.6158 - pose_output_acc: 0.6208 - emotion_output_acc: 0.7123 - val_loss: 9.0721 - val_gender_output_loss: 0.5917 - val_image_quality_output_loss: 1.1138 - val_age_output_loss: 1.3906 - val_weight_output_loss: 0.9812 - val_bag_output_loss: 0.8804 - val_footwear_output_loss: 0.8584 - val_pose_output_loss: 0.9042 - val_emotion_output_loss: 0.9005 - val_gender_output_acc: 0.6729 - val_image_quality_output_acc: 0.4466 - val_age_output_acc: 0.4194 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5832 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.5927 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 8.73483\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.8525 - gender_output_loss: 0.5789 - image_quality_output_loss: 0.9112 - age_output_loss: 1.4072 - weight_output_loss: 0.9744 - bag_output_loss: 0.8850 - footwear_output_loss: 0.8653 - pose_output_loss: 0.8921 - emotion_output_loss: 0.8942 - gender_output_acc: 0.6850 - image_quality_output_acc: 0.5617 - age_output_acc: 0.3971 - weight_output_acc: 0.6360 - bag_output_acc: 0.5794 - footwear_output_acc: 0.6080 - pose_output_acc: 0.6214 - emotion_output_acc: 0.7123 - val_loss: 9.7845 - val_gender_output_loss: 0.9879 - val_image_quality_output_loss: 1.1199 - val_age_output_loss: 1.4230 - val_weight_output_loss: 0.9898 - val_bag_output_loss: 0.9395 - val_footwear_output_loss: 1.0506 - val_pose_output_loss: 0.9127 - val_emotion_output_loss: 0.9253 - val_gender_output_acc: 0.4551 - val_image_quality_output_acc: 0.3538 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.3957 - val_footwear_output_acc: 0.4536 - val_pose_output_acc: 0.6210 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 8.73483\n",
            "Epoch 25/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.8022 - gender_output_loss: 0.5630 - image_quality_output_loss: 0.9094 - age_output_loss: 1.4058 - weight_output_loss: 0.9716 - bag_output_loss: 0.8806 - footwear_output_loss: 0.8642 - pose_output_loss: 0.8871 - emotion_output_loss: 0.8923 - gender_output_acc: 0.6999 - image_quality_output_acc: 0.5629 - age_output_acc: 0.3970 - weight_output_acc: 0.6360 - bag_output_acc: 0.5845 - footwear_output_acc: 0.6074 - pose_output_acc: 0.6232 - emotion_output_acc: 0.7123 - val_loss: 9.1972 - val_gender_output_loss: 0.7563 - val_image_quality_output_loss: 1.0323 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.9078 - val_footwear_output_loss: 0.8892 - val_pose_output_loss: 0.8850 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.5670 - val_image_quality_output_acc: 0.4783 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5136 - val_footwear_output_acc: 0.5842 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 8.73483\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.6642 - gender_output_loss: 0.5212 - image_quality_output_loss: 0.8957 - age_output_loss: 1.4024 - weight_output_loss: 0.9715 - bag_output_loss: 0.8690 - footwear_output_loss: 0.8259 - pose_output_loss: 0.8701 - emotion_output_loss: 0.8898 - gender_output_acc: 0.7431 - image_quality_output_acc: 0.5700 - age_output_acc: 0.3979 - weight_output_acc: 0.6360 - bag_output_acc: 0.5970 - footwear_output_acc: 0.6296 - pose_output_acc: 0.6306 - emotion_output_acc: 0.7123 - val_loss: 8.5956 - val_gender_output_loss: 0.4949 - val_image_quality_output_loss: 0.9026 - val_age_output_loss: 1.3800 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.8588 - val_footwear_output_loss: 0.8166 - val_pose_output_loss: 0.8554 - val_emotion_output_loss: 0.8903 - val_gender_output_acc: 0.7646 - val_image_quality_output_acc: 0.5509 - val_age_output_acc: 0.4204 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6013 - val_footwear_output_acc: 0.6351 - val_pose_output_acc: 0.6361 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00026: val_loss improved from 8.73483 to 8.59556, saving model to /content/gdrive/My Drive/model_at_epoch_26.hdf5\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 71s 197ms/step - loss: 8.6201 - gender_output_loss: 0.5039 - image_quality_output_loss: 0.8954 - age_output_loss: 1.4000 - weight_output_loss: 0.9696 - bag_output_loss: 0.8667 - footwear_output_loss: 0.8161 - pose_output_loss: 0.8656 - emotion_output_loss: 0.8875 - gender_output_acc: 0.7554 - image_quality_output_acc: 0.5658 - age_output_acc: 0.3963 - weight_output_acc: 0.6360 - bag_output_acc: 0.6056 - footwear_output_acc: 0.6332 - pose_output_acc: 0.6337 - emotion_output_acc: 0.7123 - val_loss: 8.9229 - val_gender_output_loss: 0.6829 - val_image_quality_output_loss: 0.9232 - val_age_output_loss: 1.4059 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8982 - val_footwear_output_loss: 0.8464 - val_pose_output_loss: 0.8720 - val_emotion_output_loss: 0.8973 - val_gender_output_acc: 0.6184 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.4163 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5302 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.6255 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 8.59556\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.7078 - gender_output_loss: 0.5395 - image_quality_output_loss: 0.9102 - age_output_loss: 1.4052 - weight_output_loss: 0.9724 - bag_output_loss: 0.8748 - footwear_output_loss: 0.8383 - pose_output_loss: 0.8698 - emotion_output_loss: 0.8904 - gender_output_acc: 0.7247 - image_quality_output_acc: 0.5609 - age_output_acc: 0.3989 - weight_output_acc: 0.6360 - bag_output_acc: 0.5973 - footwear_output_acc: 0.6253 - pose_output_acc: 0.6275 - emotion_output_acc: 0.7123 - val_loss: 9.1439 - val_gender_output_loss: 0.7011 - val_image_quality_output_loss: 0.9318 - val_age_output_loss: 1.4050 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9052 - val_footwear_output_loss: 0.9965 - val_pose_output_loss: 0.9127 - val_emotion_output_loss: 0.9054 - val_gender_output_acc: 0.5862 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5302 - val_footwear_output_acc: 0.5403 - val_pose_output_acc: 0.6215 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 8.59556\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.7204 - gender_output_loss: 0.5455 - image_quality_output_loss: 0.9122 - age_output_loss: 1.4032 - weight_output_loss: 0.9725 - bag_output_loss: 0.8796 - footwear_output_loss: 0.8524 - pose_output_loss: 0.8710 - emotion_output_loss: 0.8915 - gender_output_acc: 0.7205 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3951 - weight_output_acc: 0.6360 - bag_output_acc: 0.5920 - footwear_output_acc: 0.6185 - pose_output_acc: 0.6315 - emotion_output_acc: 0.7123 - val_loss: 8.7486 - val_gender_output_loss: 0.5459 - val_image_quality_output_loss: 0.9628 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.8764 - val_footwear_output_loss: 0.8575 - val_pose_output_loss: 0.8485 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.7248 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6205 - val_pose_output_acc: 0.6326 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 8.59556\n",
            "Epoch 30/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.5777 - gender_output_loss: 0.5066 - image_quality_output_loss: 0.9029 - age_output_loss: 1.4022 - weight_output_loss: 0.9703 - bag_output_loss: 0.8691 - footwear_output_loss: 0.8207 - pose_output_loss: 0.8382 - emotion_output_loss: 0.8872 - gender_output_acc: 0.7541 - image_quality_output_acc: 0.5653 - age_output_acc: 0.3972 - weight_output_acc: 0.6360 - bag_output_acc: 0.6044 - footwear_output_acc: 0.6357 - pose_output_acc: 0.6459 - emotion_output_acc: 0.7123 - val_loss: 8.6128 - val_gender_output_loss: 0.4983 - val_image_quality_output_loss: 0.9064 - val_age_output_loss: 1.3860 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.8682 - val_footwear_output_loss: 0.8343 - val_pose_output_loss: 0.8719 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.7596 - val_image_quality_output_acc: 0.5454 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6053 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.6280 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 8.59556\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.4580 - gender_output_loss: 0.4720 - image_quality_output_loss: 0.8920 - age_output_loss: 1.3975 - weight_output_loss: 0.9688 - bag_output_loss: 0.8617 - footwear_output_loss: 0.7949 - pose_output_loss: 0.8112 - emotion_output_loss: 0.8839 - gender_output_acc: 0.7779 - image_quality_output_acc: 0.5720 - age_output_acc: 0.3982 - weight_output_acc: 0.6360 - bag_output_acc: 0.6091 - footwear_output_acc: 0.6469 - pose_output_acc: 0.6509 - emotion_output_acc: 0.7123 - val_loss: 8.6972 - val_gender_output_loss: 0.5148 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.3901 - val_weight_output_loss: 0.9805 - val_bag_output_loss: 0.8695 - val_footwear_output_loss: 0.8501 - val_pose_output_loss: 0.8568 - val_emotion_output_loss: 0.8906 - val_gender_output_acc: 0.7455 - val_image_quality_output_acc: 0.5378 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6018 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.6179 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 8.59556\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.5452 - gender_output_loss: 0.5017 - image_quality_output_loss: 0.9022 - age_output_loss: 1.4006 - weight_output_loss: 0.9717 - bag_output_loss: 0.8684 - footwear_output_loss: 0.8194 - pose_output_loss: 0.8256 - emotion_output_loss: 0.8847 - gender_output_acc: 0.7594 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3970 - weight_output_acc: 0.6360 - bag_output_acc: 0.5989 - footwear_output_acc: 0.6341 - pose_output_acc: 0.6432 - emotion_output_acc: 0.7123 - val_loss: 9.3916 - val_gender_output_loss: 0.8549 - val_image_quality_output_loss: 1.0714 - val_age_output_loss: 1.4209 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.9216 - val_footwear_output_loss: 0.9732 - val_pose_output_loss: 0.8863 - val_emotion_output_loss: 0.9086 - val_gender_output_acc: 0.5247 - val_image_quality_output_acc: 0.4516 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.4970 - val_footwear_output_acc: 0.5358 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 8.59556\n",
            "Epoch 33/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.6059 - gender_output_loss: 0.5261 - image_quality_output_loss: 0.9123 - age_output_loss: 1.4028 - weight_output_loss: 0.9734 - bag_output_loss: 0.8762 - footwear_output_loss: 0.8408 - pose_output_loss: 0.8289 - emotion_output_loss: 0.8867 - gender_output_acc: 0.7368 - image_quality_output_acc: 0.5622 - age_output_acc: 0.3966 - weight_output_acc: 0.6359 - bag_output_acc: 0.5928 - footwear_output_acc: 0.6240 - pose_output_acc: 0.6399 - emotion_output_acc: 0.7123 - val_loss: 8.7617 - val_gender_output_loss: 0.6318 - val_image_quality_output_loss: 0.9396 - val_age_output_loss: 1.3910 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.8981 - val_footwear_output_loss: 0.8601 - val_pose_output_loss: 0.8097 - val_emotion_output_loss: 0.8968 - val_gender_output_acc: 0.6346 - val_image_quality_output_acc: 0.5393 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.6184 - val_pose_output_acc: 0.6391 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 8.59556\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 70s 196ms/step - loss: 8.4694 - gender_output_loss: 0.4921 - image_quality_output_loss: 0.9079 - age_output_loss: 1.3987 - weight_output_loss: 0.9722 - bag_output_loss: 0.8662 - footwear_output_loss: 0.8229 - pose_output_loss: 0.7832 - emotion_output_loss: 0.8805 - gender_output_acc: 0.7644 - image_quality_output_acc: 0.5669 - age_output_acc: 0.3991 - weight_output_acc: 0.6359 - bag_output_acc: 0.6030 - footwear_output_acc: 0.6358 - pose_output_acc: 0.6552 - emotion_output_acc: 0.7123 - val_loss: 8.4468 - val_gender_output_loss: 0.5010 - val_image_quality_output_loss: 0.9152 - val_age_output_loss: 1.3829 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8667 - val_footwear_output_loss: 0.8117 - val_pose_output_loss: 0.7592 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.7545 - val_image_quality_output_acc: 0.5398 - val_age_output_acc: 0.4194 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.6442 - val_pose_output_acc: 0.6689 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00034: val_loss improved from 8.59556 to 8.44676, saving model to /content/gdrive/My Drive/model_at_epoch_34.hdf5\n",
            "Epoch 35/50\n",
            "360/360 [==============================] - 70s 196ms/step - loss: 8.3133 - gender_output_loss: 0.4486 - image_quality_output_loss: 0.8952 - age_output_loss: 1.3913 - weight_output_loss: 0.9697 - bag_output_loss: 0.8590 - footwear_output_loss: 0.7924 - pose_output_loss: 0.7423 - emotion_output_loss: 0.8755 - gender_output_acc: 0.7936 - image_quality_output_acc: 0.5720 - age_output_acc: 0.4017 - weight_output_acc: 0.6360 - bag_output_acc: 0.6076 - footwear_output_acc: 0.6512 - pose_output_acc: 0.6697 - emotion_output_acc: 0.7123 - val_loss: 8.3914 - val_gender_output_loss: 0.4617 - val_image_quality_output_loss: 0.9092 - val_age_output_loss: 1.3787 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8620 - val_footwear_output_loss: 0.8025 - val_pose_output_loss: 0.7725 - val_emotion_output_loss: 0.8843 - val_gender_output_acc: 0.7777 - val_image_quality_output_acc: 0.5454 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6154 - val_footwear_output_acc: 0.6512 - val_pose_output_acc: 0.6658 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00035: val_loss improved from 8.44676 to 8.39135, saving model to /content/gdrive/My Drive/model_at_epoch_35.hdf5\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 71s 196ms/step - loss: 8.3349 - gender_output_loss: 0.4571 - image_quality_output_loss: 0.9023 - age_output_loss: 1.3943 - weight_output_loss: 0.9699 - bag_output_loss: 0.8570 - footwear_output_loss: 0.7996 - pose_output_loss: 0.7434 - emotion_output_loss: 0.8754 - gender_output_acc: 0.7858 - image_quality_output_acc: 0.5711 - age_output_acc: 0.3985 - weight_output_acc: 0.6359 - bag_output_acc: 0.6116 - footwear_output_acc: 0.6469 - pose_output_acc: 0.6714 - emotion_output_acc: 0.7123 - val_loss: 8.8993 - val_gender_output_loss: 0.5836 - val_image_quality_output_loss: 1.0320 - val_age_output_loss: 1.4004 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8810 - val_footwear_output_loss: 0.8514 - val_pose_output_loss: 0.9459 - val_emotion_output_loss: 0.8887 - val_gender_output_acc: 0.7127 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.4173 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5837 - val_footwear_output_acc: 0.6240 - val_pose_output_acc: 0.5570 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 8.39135\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.4616 - gender_output_loss: 0.5009 - image_quality_output_loss: 0.9126 - age_output_loss: 1.4008 - weight_output_loss: 0.9730 - bag_output_loss: 0.8682 - footwear_output_loss: 0.8204 - pose_output_loss: 0.7793 - emotion_output_loss: 0.8795 - gender_output_acc: 0.7604 - image_quality_output_acc: 0.5655 - age_output_acc: 0.3943 - weight_output_acc: 0.6364 - bag_output_acc: 0.6009 - footwear_output_acc: 0.6322 - pose_output_acc: 0.6608 - emotion_output_acc: 0.7123 - val_loss: 8.5762 - val_gender_output_loss: 0.5452 - val_image_quality_output_loss: 0.9410 - val_age_output_loss: 1.3927 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8648 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.8056 - val_emotion_output_loss: 0.8863 - val_gender_output_acc: 0.7238 - val_image_quality_output_acc: 0.5378 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6043 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.6452 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 8.39135\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.4029 - gender_output_loss: 0.4840 - image_quality_output_loss: 0.9128 - age_output_loss: 1.3975 - weight_output_loss: 0.9711 - bag_output_loss: 0.8683 - footwear_output_loss: 0.8228 - pose_output_loss: 0.7538 - emotion_output_loss: 0.8786 - gender_output_acc: 0.7688 - image_quality_output_acc: 0.5655 - age_output_acc: 0.3957 - weight_output_acc: 0.6363 - bag_output_acc: 0.6034 - footwear_output_acc: 0.6378 - pose_output_acc: 0.6715 - emotion_output_acc: 0.7123 - val_loss: 8.5509 - val_gender_output_loss: 0.4998 - val_image_quality_output_loss: 0.9383 - val_age_output_loss: 1.3820 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.8694 - val_footwear_output_loss: 0.8783 - val_pose_output_loss: 0.7865 - val_emotion_output_loss: 0.9062 - val_gender_output_acc: 0.7500 - val_image_quality_output_acc: 0.5328 - val_age_output_acc: 0.4168 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.6371 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 8.39135\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.2243 - gender_output_loss: 0.4376 - image_quality_output_loss: 0.9002 - age_output_loss: 1.3925 - weight_output_loss: 0.9693 - bag_output_loss: 0.8508 - footwear_output_loss: 0.7884 - pose_output_loss: 0.7080 - emotion_output_loss: 0.8720 - gender_output_acc: 0.7990 - image_quality_output_acc: 0.5757 - age_output_acc: 0.4019 - weight_output_acc: 0.6361 - bag_output_acc: 0.6167 - footwear_output_acc: 0.6532 - pose_output_acc: 0.6893 - emotion_output_acc: 0.7123 - val_loss: 8.4026 - val_gender_output_loss: 0.4758 - val_image_quality_output_loss: 0.9316 - val_age_output_loss: 1.3828 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.8683 - val_footwear_output_loss: 0.8170 - val_pose_output_loss: 0.7585 - val_emotion_output_loss: 0.8820 - val_gender_output_acc: 0.7722 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6048 - val_footwear_output_acc: 0.6568 - val_pose_output_acc: 0.6830 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 8.39135\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.1362 - gender_output_loss: 0.4141 - image_quality_output_loss: 0.8960 - age_output_loss: 1.3888 - weight_output_loss: 0.9686 - bag_output_loss: 0.8474 - footwear_output_loss: 0.7669 - pose_output_loss: 0.6841 - emotion_output_loss: 0.8677 - gender_output_acc: 0.8154 - image_quality_output_acc: 0.5753 - age_output_acc: 0.4007 - weight_output_acc: 0.6362 - bag_output_acc: 0.6221 - footwear_output_acc: 0.6653 - pose_output_acc: 0.7023 - emotion_output_acc: 0.7123 - val_loss: 8.3927 - val_gender_output_loss: 0.4772 - val_image_quality_output_loss: 0.9358 - val_age_output_loss: 1.3849 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8637 - val_footwear_output_loss: 0.8209 - val_pose_output_loss: 0.7441 - val_emotion_output_loss: 0.8819 - val_gender_output_acc: 0.7752 - val_image_quality_output_acc: 0.5348 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.6835 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 8.39135\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.2765 - gender_output_loss: 0.4570 - image_quality_output_loss: 0.9052 - age_output_loss: 1.3917 - weight_output_loss: 0.9704 - bag_output_loss: 0.8600 - footwear_output_loss: 0.8015 - pose_output_loss: 0.7201 - emotion_output_loss: 0.8746 - gender_output_acc: 0.7869 - image_quality_output_acc: 0.5727 - age_output_acc: 0.3997 - weight_output_acc: 0.6357 - bag_output_acc: 0.6064 - footwear_output_acc: 0.6460 - pose_output_acc: 0.6875 - emotion_output_acc: 0.7123 - val_loss: 8.5679 - val_gender_output_loss: 0.5173 - val_image_quality_output_loss: 0.9241 - val_age_output_loss: 1.3909 - val_weight_output_loss: 0.9837 - val_bag_output_loss: 0.8730 - val_footwear_output_loss: 0.8228 - val_pose_output_loss: 0.8727 - val_emotion_output_loss: 0.8925 - val_gender_output_acc: 0.7445 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.4189 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5968 - val_footwear_output_acc: 0.6381 - val_pose_output_acc: 0.5902 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 8.39135\n",
            "Epoch 42/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.3260 - gender_output_loss: 0.4742 - image_quality_output_loss: 0.9176 - age_output_loss: 1.3974 - weight_output_loss: 0.9719 - bag_output_loss: 0.8630 - footwear_output_loss: 0.8146 - pose_output_loss: 0.7248 - emotion_output_loss: 0.8778 - gender_output_acc: 0.7740 - image_quality_output_acc: 0.5657 - age_output_acc: 0.4002 - weight_output_acc: 0.6359 - bag_output_acc: 0.6031 - footwear_output_acc: 0.6410 - pose_output_acc: 0.6879 - emotion_output_acc: 0.7123 - val_loss: 8.3496 - val_gender_output_loss: 0.4857 - val_image_quality_output_loss: 0.9258 - val_age_output_loss: 1.3859 - val_weight_output_loss: 0.9832 - val_bag_output_loss: 0.8624 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.7073 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.7692 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6053 - val_footwear_output_acc: 0.6366 - val_pose_output_acc: 0.6941 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00042: val_loss improved from 8.39135 to 8.34958, saving model to /content/gdrive/My Drive/model_at_epoch_42.hdf5\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.1552 - gender_output_loss: 0.4321 - image_quality_output_loss: 0.9029 - age_output_loss: 1.3895 - weight_output_loss: 0.9693 - bag_output_loss: 0.8512 - footwear_output_loss: 0.7887 - pose_output_loss: 0.6799 - emotion_output_loss: 0.8668 - gender_output_acc: 0.8012 - image_quality_output_acc: 0.5740 - age_output_acc: 0.3978 - weight_output_acc: 0.6363 - bag_output_acc: 0.6159 - footwear_output_acc: 0.6510 - pose_output_acc: 0.7092 - emotion_output_acc: 0.7123 - val_loss: 8.5259 - val_gender_output_loss: 0.4832 - val_image_quality_output_loss: 0.9359 - val_age_output_loss: 1.3819 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8861 - val_footwear_output_loss: 0.8490 - val_pose_output_loss: 0.8454 - val_emotion_output_loss: 0.8896 - val_gender_output_acc: 0.7702 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.4138 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5932 - val_footwear_output_acc: 0.6386 - val_pose_output_acc: 0.6159 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 8.34958\n",
            "Epoch 44/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 7.9912 - gender_output_loss: 0.3815 - image_quality_output_loss: 0.8869 - age_output_loss: 1.3861 - weight_output_loss: 0.9668 - bag_output_loss: 0.8382 - footwear_output_loss: 0.7552 - pose_output_loss: 0.6444 - emotion_output_loss: 0.8611 - gender_output_acc: 0.8327 - image_quality_output_acc: 0.5848 - age_output_acc: 0.3977 - weight_output_acc: 0.6374 - bag_output_acc: 0.6293 - footwear_output_acc: 0.6697 - pose_output_acc: 0.7264 - emotion_output_acc: 0.7123 - val_loss: 8.5974 - val_gender_output_loss: 0.5711 - val_image_quality_output_loss: 0.9263 - val_age_output_loss: 1.3853 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8922 - val_footwear_output_loss: 0.8175 - val_pose_output_loss: 0.8552 - val_emotion_output_loss: 0.8971 - val_gender_output_acc: 0.7359 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.4153 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6018 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.6089 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 8.34958\n",
            "Epoch 45/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.0764 - gender_output_loss: 0.4145 - image_quality_output_loss: 0.8955 - age_output_loss: 1.3879 - weight_output_loss: 0.9695 - bag_output_loss: 0.8461 - footwear_output_loss: 0.7715 - pose_output_loss: 0.6602 - emotion_output_loss: 0.8645 - gender_output_acc: 0.8127 - image_quality_output_acc: 0.5779 - age_output_acc: 0.4023 - weight_output_acc: 0.6372 - bag_output_acc: 0.6213 - footwear_output_acc: 0.6620 - pose_output_acc: 0.7168 - emotion_output_acc: 0.7123 - val_loss: 8.8590 - val_gender_output_loss: 0.5515 - val_image_quality_output_loss: 0.9830 - val_age_output_loss: 1.3948 - val_weight_output_loss: 0.9824 - val_bag_output_loss: 0.8766 - val_footwear_output_loss: 0.8899 - val_pose_output_loss: 0.9743 - val_emotion_output_loss: 0.9436 - val_gender_output_acc: 0.7092 - val_image_quality_output_acc: 0.5066 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.5731 - val_pose_output_acc: 0.5403 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 8.34958\n",
            "Epoch 46/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 8.2376 - gender_output_loss: 0.4631 - image_quality_output_loss: 0.9082 - age_output_loss: 1.3943 - weight_output_loss: 0.9723 - bag_output_loss: 0.8604 - footwear_output_loss: 0.8036 - pose_output_loss: 0.7065 - emotion_output_loss: 0.8715 - gender_output_acc: 0.7843 - image_quality_output_acc: 0.5675 - age_output_acc: 0.3964 - weight_output_acc: 0.6360 - bag_output_acc: 0.6084 - footwear_output_acc: 0.6464 - pose_output_acc: 0.6971 - emotion_output_acc: 0.7123 - val_loss: 8.7168 - val_gender_output_loss: 0.5621 - val_image_quality_output_loss: 0.9157 - val_age_output_loss: 1.3992 - val_weight_output_loss: 0.9900 - val_bag_output_loss: 0.8918 - val_footwear_output_loss: 0.8937 - val_pose_output_loss: 0.9242 - val_emotion_output_loss: 0.8880 - val_gender_output_acc: 0.7122 - val_image_quality_output_acc: 0.5338 - val_age_output_acc: 0.4178 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5746 - val_footwear_output_acc: 0.5998 - val_pose_output_acc: 0.6406 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 8.34958\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.0935 - gender_output_loss: 0.4229 - image_quality_output_loss: 0.9039 - age_output_loss: 1.3900 - weight_output_loss: 0.9691 - bag_output_loss: 0.8530 - footwear_output_loss: 0.7799 - pose_output_loss: 0.6605 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8095 - image_quality_output_acc: 0.5724 - age_output_acc: 0.4009 - weight_output_acc: 0.6362 - bag_output_acc: 0.6149 - footwear_output_acc: 0.6584 - pose_output_acc: 0.7174 - emotion_output_acc: 0.7123 - val_loss: 8.2894 - val_gender_output_loss: 0.4515 - val_image_quality_output_loss: 0.9062 - val_age_output_loss: 1.3822 - val_weight_output_loss: 0.9831 - val_bag_output_loss: 0.8577 - val_footwear_output_loss: 0.8464 - val_pose_output_loss: 0.7345 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.7939 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4189 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6104 - val_footwear_output_acc: 0.6316 - val_pose_output_acc: 0.6714 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00047: val_loss improved from 8.34958 to 8.28936, saving model to /content/gdrive/My Drive/model_at_epoch_47.hdf5\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 7.8926 - gender_output_loss: 0.3604 - image_quality_output_loss: 0.8822 - age_output_loss: 1.3862 - weight_output_loss: 0.9662 - bag_output_loss: 0.8341 - footwear_output_loss: 0.7466 - pose_output_loss: 0.6159 - emotion_output_loss: 0.8592 - gender_output_acc: 0.8467 - image_quality_output_acc: 0.5869 - age_output_acc: 0.4011 - weight_output_acc: 0.6373 - bag_output_acc: 0.6326 - footwear_output_acc: 0.6782 - pose_output_acc: 0.7384 - emotion_output_acc: 0.7123 - val_loss: 8.1683 - val_gender_output_loss: 0.4164 - val_image_quality_output_loss: 0.9059 - val_age_output_loss: 1.3767 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8519 - val_footwear_output_loss: 0.8169 - val_pose_output_loss: 0.7019 - val_emotion_output_loss: 0.8749 - val_gender_output_acc: 0.8004 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.4209 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6285 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.6890 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00048: val_loss improved from 8.28936 to 8.16828, saving model to /content/gdrive/My Drive/model_at_epoch_48.hdf5\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 70s 195ms/step - loss: 7.8802 - gender_output_loss: 0.3571 - image_quality_output_loss: 0.8856 - age_output_loss: 1.3812 - weight_output_loss: 0.9670 - bag_output_loss: 0.8362 - footwear_output_loss: 0.7463 - pose_output_loss: 0.6105 - emotion_output_loss: 0.8574 - gender_output_acc: 0.8461 - image_quality_output_acc: 0.5811 - age_output_acc: 0.4031 - weight_output_acc: 0.6374 - bag_output_acc: 0.6281 - footwear_output_acc: 0.6739 - pose_output_acc: 0.7435 - emotion_output_acc: 0.7123 - val_loss: 8.2670 - val_gender_output_loss: 0.4432 - val_image_quality_output_loss: 0.9184 - val_age_output_loss: 1.3841 - val_weight_output_loss: 0.9839 - val_bag_output_loss: 0.8616 - val_footwear_output_loss: 0.8265 - val_pose_output_loss: 0.7278 - val_emotion_output_loss: 0.8853 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.4244 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.6119 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.6613 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 8.16828\n",
            "Epoch 50/50\n",
            "360/360 [==============================] - 70s 194ms/step - loss: 8.0945 - gender_output_loss: 0.4246 - image_quality_output_loss: 0.9015 - age_output_loss: 1.3943 - weight_output_loss: 0.9716 - bag_output_loss: 0.8499 - footwear_output_loss: 0.7845 - pose_output_loss: 0.6666 - emotion_output_loss: 0.8692 - gender_output_acc: 0.8059 - image_quality_output_acc: 0.5763 - age_output_acc: 0.3992 - weight_output_acc: 0.6360 - bag_output_acc: 0.6230 - footwear_output_acc: 0.6555 - pose_output_acc: 0.7159 - emotion_output_acc: 0.7123 - val_loss: 8.5600 - val_gender_output_loss: 0.5761 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8878 - val_footwear_output_loss: 0.8548 - val_pose_output_loss: 0.7564 - val_emotion_output_loss: 0.8931 - val_gender_output_acc: 0.7046 - val_image_quality_output_acc: 0.5111 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.6689 - val_emotion_output_acc: 0.7097\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 8.16828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f33424a9ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBPDhJ27sQyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "1061f0f5-2584-41f4-d8b6-8a4d947aa9c4"
      },
      "source": [
        "def evaluate_model(model):\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    accuracies = {}\n",
        "    losses = {}\n",
        "    for k, v in zip(model.metrics_names, results):\n",
        "        if k.endswith('_acc'):\n",
        "            accuracies[k] = round(v * 100, 4) \n",
        "        else:\n",
        "            losses[k] = v\n",
        "    return accuracies\n",
        "    \n",
        "evaluate_model(model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 3s 95ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 41.5827,\n",
              " 'bag_output_acc': 58.1653,\n",
              " 'emotion_output_acc': 70.9677,\n",
              " 'footwear_output_acc': 62.8528,\n",
              " 'gender_output_acc': 70.4637,\n",
              " 'image_quality_output_acc': 51.1089,\n",
              " 'pose_output_acc': 66.8851,\n",
              " 'weight_output_acc': 63.0544}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}