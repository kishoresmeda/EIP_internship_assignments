{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5th Assignment",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kishoresmeda/EIP_internship_work/blob/master/5th%20Assignment/5th_Assignment_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f13d2463-52fc-476b-e8f6-ff008d6319c3"
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Tu5IEpEZ1j",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0dae31d9-6b8b-48ff-9efc-f99a39a3abb9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "067e8e08-8ede-4120-c203-443cf51a6736"
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBTOzYO0seQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df['image_path'] = 'hvc_data/' + df['image_path'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvLdDBGHHdby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c622cbed-9a9e-4e58-bcdd-97df50752b65"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "outputId": "6763bafe-7cb4-4876-a911-354301f7bac9"
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            images = self.augmentation.flow(images, shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9234494b-158d-43dc-e3c7-74f303bd3a63"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "0ca62c1b-b5e6-4e27-92ce-4b8588de0e34"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12916</th>\n",
              "      <td>resized/12918.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9706</th>\n",
              "      <td>resized/9707.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8348</th>\n",
              "      <td>resized/8349.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5531</th>\n",
              "      <td>resized/5532.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11636</th>\n",
              "      <td>resized/11638.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "12916  resized/12918.jpg              1  ...                        1              0\n",
              "9706    resized/9707.jpg              1  ...                        1              0\n",
              "8348    resized/8349.jpg              0  ...                        1              0\n",
              "5531    resized/5532.jpg              1  ...                        0              0\n",
              "11636  resized/11638.jpg              0  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "# train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=32, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "    )\n",
        ")\n",
        "valid_gen = PersonDataGenerator(train_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "c3431ace-3a1a-4cf7-bae8-bb5d036ed1ca"
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4WgGYl_mSOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93f37867-ccbe-414f-c3ad-0d55b70bafcd"
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6-sDJvmmVk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63c8e12b-1a63-4a15-ee89-42a35b17f3ea"
      },
      "source": [
        "len(targets)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_PK9ixLrQ_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Triangle cyclic learning rate policy\n",
        "\n",
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.1,step_size=782.)\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = '/content/gdrive/My Drive/model_at_epoch_{epoch}.hdf5'\n",
        "ckpt =ModelCheckpoint(filepath, verbose=1, monitor='val_loss', save_best_only=True,save_weights_only=False, mode='auto',period=1)\n",
        "callback = [ckpt,clr]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD0A77DRrlCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Referred from https://github.com/raghakot/keras-resnet/blob/master/resnet.py\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.common.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uKk6wWar53N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "b570d191-9636-4435-eab1-88a9747d5590"
      },
      "source": [
        "backbone = ResnetBuilder.build_resnet_34((3,224,224), 8)\n",
        "\n",
        "neck = backbone.output\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET0mczj1sDCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "601a889d-dafa-410a-c15e-408d92d64090"
      },
      "source": [
        "losses = {\"gender_output\": \"binary_crossentropy\",\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\"age_output\": \"categorical_crossentropy\",\n",
        "\"weight_output\": \"categorical_crossentropy\",\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.1,\n",
        "                        step_size=782.)\n",
        "from keras import optimizers\n",
        "opt = keras.optimizers.SGD(lr=0.001,momentum=1e-2)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfGdwKl8sKlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "479de670-643f-4aa4-9e20-a374651a1a0d"
      },
      "source": [
        "model.fit_generator(generator=train_gen, validation_data=valid_gen, use_multiprocessing = True,\n",
        "    workers=-1, epochs=50, verbose=1, callbacks = [clr, ckpt])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "360/360 [==============================] - 100s 278ms/step - loss: 9.9623 - gender_output_loss: 0.6866 - image_quality_output_loss: 1.0061 - age_output_loss: 1.4804 - weight_output_loss: 1.0827 - bag_output_loss: 0.9645 - footwear_output_loss: 1.0555 - pose_output_loss: 0.9638 - emotion_output_loss: 1.0215 - gender_output_acc: 0.5633 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3971 - weight_output_acc: 0.6141 - bag_output_acc: 0.5495 - footwear_output_acc: 0.4427 - pose_output_acc: 0.5973 - emotion_output_acc: 0.6948 - val_loss: 9.5616 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9765 - val_age_output_loss: 1.4280 - val_weight_output_loss: 0.9844 - val_bag_output_loss: 0.9160 - val_footwear_output_loss: 1.0366 - val_pose_output_loss: 0.9283 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.4431 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 9.56163, saving model to /content/gdrive/My Drive/model_at_epoch_1.hdf5\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.5249 - gender_output_loss: 0.6854 - image_quality_output_loss: 0.9717 - age_output_loss: 1.4284 - weight_output_loss: 0.9848 - bag_output_loss: 0.9165 - footwear_output_loss: 1.0118 - pose_output_loss: 0.9271 - emotion_output_loss: 0.9096 - gender_output_acc: 0.5644 - image_quality_output_acc: 0.5564 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5028 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.5872 - val_gender_output_loss: 0.6847 - val_image_quality_output_loss: 0.9690 - val_age_output_loss: 1.4280 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9142 - val_footwear_output_loss: 1.0780 - val_pose_output_loss: 0.9335 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.4665 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 9.56163\n",
            "Epoch 3/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 9.4315 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9490 - age_output_loss: 1.4274 - weight_output_loss: 0.9830 - bag_output_loss: 0.9145 - footwear_output_loss: 0.9723 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9071 - gender_output_acc: 0.5652 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5453 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.5708 - val_gender_output_loss: 0.6883 - val_image_quality_output_loss: 1.0439 - val_age_output_loss: 1.4275 - val_weight_output_loss: 0.9828 - val_bag_output_loss: 0.9157 - val_footwear_output_loss: 1.0162 - val_pose_output_loss: 0.9285 - val_emotion_output_loss: 0.9078 - val_gender_output_acc: 0.5629 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.4928 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 9.56163\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.3664 - gender_output_loss: 0.6818 - image_quality_output_loss: 0.9246 - age_output_loss: 1.4262 - weight_output_loss: 0.9805 - bag_output_loss: 0.9128 - footwear_output_loss: 0.9541 - pose_output_loss: 0.9266 - emotion_output_loss: 0.9053 - gender_output_acc: 0.5687 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5536 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.4799 - val_gender_output_loss: 0.6855 - val_image_quality_output_loss: 1.0105 - val_age_output_loss: 1.4256 - val_weight_output_loss: 0.9819 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9793 - val_pose_output_loss: 0.9263 - val_emotion_output_loss: 0.9074 - val_gender_output_acc: 0.5727 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5247 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00004: val_loss improved from 9.56163 to 9.47993, saving model to /content/gdrive/My Drive/model_at_epoch_4.hdf5\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 9.3249 - gender_output_loss: 0.6798 - image_quality_output_loss: 0.9122 - age_output_loss: 1.4242 - weight_output_loss: 0.9790 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9401 - pose_output_loss: 0.9251 - emotion_output_loss: 0.9035 - gender_output_acc: 0.5766 - image_quality_output_acc: 0.5564 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5632 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.5010 - val_gender_output_loss: 0.6869 - val_image_quality_output_loss: 1.0414 - val_age_output_loss: 1.4258 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9706 - val_pose_output_loss: 0.9268 - val_emotion_output_loss: 0.9075 - val_gender_output_acc: 0.5635 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5345 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 9.47993\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.3356 - gender_output_loss: 0.6804 - image_quality_output_loss: 0.9176 - age_output_loss: 1.4243 - weight_output_loss: 0.9810 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9472 - pose_output_loss: 0.9257 - emotion_output_loss: 0.9050 - gender_output_acc: 0.5737 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5565 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.4791 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 0.9474 - val_age_output_loss: 1.4262 - val_weight_output_loss: 0.9834 - val_bag_output_loss: 0.9136 - val_footwear_output_loss: 1.0451 - val_pose_output_loss: 0.9292 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.5648 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.4791 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00006: val_loss improved from 9.47993 to 9.47909, saving model to /content/gdrive/My Drive/model_at_epoch_6.hdf5\n",
            "Epoch 7/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 9.3219 - gender_output_loss: 0.6781 - image_quality_output_loss: 0.9219 - age_output_loss: 1.4246 - weight_output_loss: 0.9810 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9483 - pose_output_loss: 0.9258 - emotion_output_loss: 0.9059 - gender_output_acc: 0.5747 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5591 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.6313 - val_gender_output_loss: 0.6837 - val_image_quality_output_loss: 1.0720 - val_age_output_loss: 1.4313 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9163 - val_footwear_output_loss: 1.0807 - val_pose_output_loss: 0.9354 - val_emotion_output_loss: 0.9134 - val_gender_output_acc: 0.5656 - val_image_quality_output_acc: 0.5567 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.4727 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 9.47909\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.2617 - gender_output_loss: 0.6733 - image_quality_output_loss: 0.9124 - age_output_loss: 1.4201 - weight_output_loss: 0.9784 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9321 - pose_output_loss: 0.9244 - emotion_output_loss: 0.9046 - gender_output_acc: 0.5856 - image_quality_output_acc: 0.5591 - age_output_acc: 0.3975 - weight_output_acc: 0.6344 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5693 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.4763 - val_gender_output_loss: 0.6982 - val_image_quality_output_loss: 1.0912 - val_age_output_loss: 1.4263 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.9111 - val_footwear_output_loss: 0.9319 - val_pose_output_loss: 0.9269 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.5024 - val_image_quality_output_acc: 0.5160 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5667 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00008: val_loss improved from 9.47909 to 9.47634, saving model to /content/gdrive/My Drive/model_at_epoch_8.hdf5\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 9.2038 - gender_output_loss: 0.6664 - image_quality_output_loss: 0.9025 - age_output_loss: 1.4178 - weight_output_loss: 0.9768 - bag_output_loss: 0.9066 - footwear_output_loss: 0.9098 - pose_output_loss: 0.9217 - emotion_output_loss: 0.9028 - gender_output_acc: 0.5927 - image_quality_output_acc: 0.5622 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5812 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.3734 - val_gender_output_loss: 0.6794 - val_image_quality_output_loss: 1.0285 - val_age_output_loss: 1.4225 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.9088 - val_footwear_output_loss: 0.9254 - val_pose_output_loss: 0.9231 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.5755 - val_image_quality_output_acc: 0.5185 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5707 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00009: val_loss improved from 9.47634 to 9.37345, saving model to /content/gdrive/My Drive/model_at_epoch_9.hdf5\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 9.2076 - gender_output_loss: 0.6641 - image_quality_output_loss: 0.9053 - age_output_loss: 1.4171 - weight_output_loss: 0.9770 - bag_output_loss: 0.9063 - footwear_output_loss: 0.9179 - pose_output_loss: 0.9217 - emotion_output_loss: 0.9034 - gender_output_acc: 0.5948 - image_quality_output_acc: 0.5585 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5647 - footwear_output_acc: 0.5773 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.3809 - val_gender_output_loss: 0.6975 - val_image_quality_output_loss: 0.9475 - val_age_output_loss: 1.4341 - val_weight_output_loss: 0.9859 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 0.9731 - val_pose_output_loss: 0.9290 - val_emotion_output_loss: 0.9061 - val_gender_output_acc: 0.5138 - val_image_quality_output_acc: 0.5530 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5272 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 9.37345\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 9.2192 - gender_output_loss: 0.6624 - image_quality_output_loss: 0.9186 - age_output_loss: 1.4182 - weight_output_loss: 0.9781 - bag_output_loss: 0.9060 - footwear_output_loss: 0.9285 - pose_output_loss: 0.9222 - emotion_output_loss: 0.9041 - gender_output_acc: 0.5980 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3975 - weight_output_acc: 0.6344 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5714 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.3792 - val_gender_output_loss: 0.6766 - val_image_quality_output_loss: 0.9458 - val_age_output_loss: 1.4338 - val_weight_output_loss: 0.9870 - val_bag_output_loss: 0.9155 - val_footwear_output_loss: 1.0112 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9066 - val_gender_output_acc: 0.5677 - val_image_quality_output_acc: 0.5593 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5065 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 9.37345\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 9.1626 - gender_output_loss: 0.6535 - image_quality_output_loss: 0.9159 - age_output_loss: 1.4165 - weight_output_loss: 0.9779 - bag_output_loss: 0.9028 - footwear_output_loss: 0.9101 - pose_output_loss: 0.9199 - emotion_output_loss: 0.9035 - gender_output_acc: 0.6066 - image_quality_output_acc: 0.5597 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5675 - footwear_output_acc: 0.5805 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.2879 - val_gender_output_loss: 0.6657 - val_image_quality_output_loss: 0.9061 - val_age_output_loss: 1.4216 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.9045 - val_footwear_output_loss: 1.0216 - val_pose_output_loss: 0.9342 - val_emotion_output_loss: 0.9007 - val_gender_output_acc: 0.6028 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5634 - val_footwear_output_acc: 0.5266 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00012: val_loss improved from 9.37345 to 9.28793, saving model to /content/gdrive/My Drive/model_at_epoch_12.hdf5\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 9.0639 - gender_output_loss: 0.6292 - image_quality_output_loss: 0.9038 - age_output_loss: 1.4108 - weight_output_loss: 0.9755 - bag_output_loss: 0.8940 - footwear_output_loss: 0.8823 - pose_output_loss: 0.9156 - emotion_output_loss: 0.9010 - gender_output_acc: 0.6379 - image_quality_output_acc: 0.5638 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5740 - footwear_output_acc: 0.5990 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.9673 - val_gender_output_loss: 0.6007 - val_image_quality_output_loss: 0.8860 - val_age_output_loss: 1.4066 - val_weight_output_loss: 0.9734 - val_bag_output_loss: 0.8870 - val_footwear_output_loss: 0.8559 - val_pose_output_loss: 0.9103 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.6841 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.6103 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00013: val_loss improved from 9.28793 to 8.96728, saving model to /content/gdrive/My Drive/model_at_epoch_13.hdf5\n",
            "Epoch 14/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.0312 - gender_output_loss: 0.6171 - image_quality_output_loss: 0.9028 - age_output_loss: 1.4088 - weight_output_loss: 0.9742 - bag_output_loss: 0.8912 - footwear_output_loss: 0.8732 - pose_output_loss: 0.9152 - emotion_output_loss: 0.9005 - gender_output_acc: 0.6549 - image_quality_output_acc: 0.5695 - age_output_acc: 0.3979 - weight_output_acc: 0.6344 - bag_output_acc: 0.5766 - footwear_output_acc: 0.6066 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.4113 - val_gender_output_loss: 0.6643 - val_image_quality_output_loss: 1.0011 - val_age_output_loss: 1.4342 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9109 - val_footwear_output_loss: 1.0253 - val_pose_output_loss: 0.9354 - val_emotion_output_loss: 0.9096 - val_gender_output_acc: 0.5841 - val_image_quality_output_acc: 0.5582 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5561 - val_footwear_output_acc: 0.5376 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 8.96728\n",
            "Epoch 15/50\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 9.0802 - gender_output_loss: 0.6334 - image_quality_output_loss: 0.9119 - age_output_loss: 1.4124 - weight_output_loss: 0.9773 - bag_output_loss: 0.8943 - footwear_output_loss: 0.8941 - pose_output_loss: 0.9162 - emotion_output_loss: 0.9022 - gender_output_acc: 0.6328 - image_quality_output_acc: 0.5646 - age_output_acc: 0.3981 - weight_output_acc: 0.6344 - bag_output_acc: 0.5753 - footwear_output_acc: 0.5865 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.7315 - val_gender_output_loss: 0.6870 - val_image_quality_output_loss: 1.0867 - val_age_output_loss: 1.4467 - val_weight_output_loss: 0.9985 - val_bag_output_loss: 0.9258 - val_footwear_output_loss: 1.1776 - val_pose_output_loss: 0.9563 - val_emotion_output_loss: 0.9224 - val_gender_output_acc: 0.5701 - val_image_quality_output_acc: 0.5574 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5555 - val_footwear_output_acc: 0.4860 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 8.96728\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 9.0526 - gender_output_loss: 0.6244 - image_quality_output_loss: 0.9163 - age_output_loss: 1.4132 - weight_output_loss: 0.9779 - bag_output_loss: 0.8934 - footwear_output_loss: 0.8885 - pose_output_loss: 0.9155 - emotion_output_loss: 0.9024 - gender_output_acc: 0.6420 - image_quality_output_acc: 0.5595 - age_output_acc: 0.3968 - weight_output_acc: 0.6344 - bag_output_acc: 0.5755 - footwear_output_acc: 0.5908 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.6652 - val_gender_output_loss: 0.7279 - val_image_quality_output_loss: 1.0643 - val_age_output_loss: 1.4514 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.9327 - val_footwear_output_loss: 1.1274 - val_pose_output_loss: 0.9399 - val_emotion_output_loss: 0.9181 - val_gender_output_acc: 0.4428 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.4781 - val_footwear_output_acc: 0.4931 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 8.96728\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 8.9452 - gender_output_loss: 0.5911 - image_quality_output_loss: 0.9085 - age_output_loss: 1.4071 - weight_output_loss: 0.9764 - bag_output_loss: 0.8849 - footwear_output_loss: 0.8603 - pose_output_loss: 0.9108 - emotion_output_loss: 0.8986 - gender_output_acc: 0.6781 - image_quality_output_acc: 0.5671 - age_output_acc: 0.3989 - weight_output_acc: 0.6344 - bag_output_acc: 0.5816 - footwear_output_acc: 0.6066 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.9141 - val_gender_output_loss: 0.6078 - val_image_quality_output_loss: 0.8935 - val_age_output_loss: 1.4126 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.8875 - val_footwear_output_loss: 0.8278 - val_pose_output_loss: 0.9065 - val_emotion_output_loss: 0.8973 - val_gender_output_acc: 0.6637 - val_image_quality_output_acc: 0.5684 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5809 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00017: val_loss improved from 8.96728 to 8.91407, saving model to /content/gdrive/My Drive/model_at_epoch_17.hdf5\n",
            "Epoch 18/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 8.8676 - gender_output_loss: 0.5682 - image_quality_output_loss: 0.9031 - age_output_loss: 1.4036 - weight_output_loss: 0.9743 - bag_output_loss: 0.8754 - footwear_output_loss: 0.8373 - pose_output_loss: 0.9067 - emotion_output_loss: 0.8962 - gender_output_acc: 0.7000 - image_quality_output_acc: 0.5676 - age_output_acc: 0.3984 - weight_output_acc: 0.6344 - bag_output_acc: 0.5887 - footwear_output_acc: 0.6227 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.2160 - val_gender_output_loss: 0.6897 - val_image_quality_output_loss: 1.0095 - val_age_output_loss: 1.4259 - val_weight_output_loss: 0.9803 - val_bag_output_loss: 0.9135 - val_footwear_output_loss: 0.8649 - val_pose_output_loss: 0.9191 - val_emotion_output_loss: 0.9119 - val_gender_output_acc: 0.5627 - val_image_quality_output_acc: 0.5478 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5264 - val_footwear_output_acc: 0.6265 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 8.91407\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.9194 - gender_output_loss: 0.5885 - image_quality_output_loss: 0.9111 - age_output_loss: 1.4075 - weight_output_loss: 0.9757 - bag_output_loss: 0.8805 - footwear_output_loss: 0.8528 - pose_output_loss: 0.9083 - emotion_output_loss: 0.8984 - gender_output_acc: 0.6796 - image_quality_output_acc: 0.5638 - age_output_acc: 0.3967 - weight_output_acc: 0.6344 - bag_output_acc: 0.5861 - footwear_output_acc: 0.6122 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.3046 - val_gender_output_loss: 0.7474 - val_image_quality_output_loss: 1.0089 - val_age_output_loss: 1.4318 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.9295 - val_footwear_output_loss: 0.8841 - val_pose_output_loss: 0.9098 - val_emotion_output_loss: 0.9171 - val_gender_output_acc: 0.6037 - val_image_quality_output_acc: 0.5582 - val_age_output_acc: 0.3970 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.5960 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 8.91407\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 8.9516 - gender_output_loss: 0.5996 - image_quality_output_loss: 0.9194 - age_output_loss: 1.4122 - weight_output_loss: 0.9777 - bag_output_loss: 0.8860 - footwear_output_loss: 0.8651 - pose_output_loss: 0.9090 - emotion_output_loss: 0.9008 - gender_output_acc: 0.6766 - image_quality_output_acc: 0.5607 - age_output_acc: 0.3979 - weight_output_acc: 0.6344 - bag_output_acc: 0.5836 - footwear_output_acc: 0.6039 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.2130 - val_gender_output_loss: 0.6860 - val_image_quality_output_loss: 1.0105 - val_age_output_loss: 1.4290 - val_weight_output_loss: 0.9838 - val_bag_output_loss: 0.9161 - val_footwear_output_loss: 0.8803 - val_pose_output_loss: 0.9165 - val_emotion_output_loss: 0.9178 - val_gender_output_acc: 0.5382 - val_image_quality_output_acc: 0.5473 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5504 - val_footwear_output_acc: 0.6083 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 8.91407\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 8.8496 - gender_output_loss: 0.5639 - image_quality_output_loss: 0.9118 - age_output_loss: 1.4070 - weight_output_loss: 0.9764 - bag_output_loss: 0.8791 - footwear_output_loss: 0.8417 - pose_output_loss: 0.9054 - emotion_output_loss: 0.8979 - gender_output_acc: 0.7083 - image_quality_output_acc: 0.5625 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5846 - footwear_output_acc: 0.6227 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.7552 - val_gender_output_loss: 0.5495 - val_image_quality_output_loss: 0.8864 - val_age_output_loss: 1.4070 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8745 - val_footwear_output_loss: 0.8122 - val_pose_output_loss: 0.8940 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.7194 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.6353 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00021: val_loss improved from 8.91407 to 8.75525, saving model to /content/gdrive/My Drive/model_at_epoch_21.hdf5\n",
            "Epoch 22/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.7380 - gender_output_loss: 0.5264 - image_quality_output_loss: 0.8995 - age_output_loss: 1.4018 - weight_output_loss: 0.9738 - bag_output_loss: 0.8668 - footwear_output_loss: 0.8175 - pose_output_loss: 0.8995 - emotion_output_loss: 0.8932 - gender_output_acc: 0.7325 - image_quality_output_acc: 0.5710 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.6002 - footwear_output_acc: 0.6386 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.6427 - val_gender_output_loss: 0.5139 - val_image_quality_output_loss: 0.8762 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9727 - val_bag_output_loss: 0.8629 - val_footwear_output_loss: 0.7795 - val_pose_output_loss: 0.8872 - val_emotion_output_loss: 0.8919 - val_gender_output_acc: 0.7477 - val_image_quality_output_acc: 0.5782 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6575 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00022: val_loss improved from 8.75525 to 8.64272, saving model to /content/gdrive/My Drive/model_at_epoch_22.hdf5\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 91s 251ms/step - loss: 8.7507 - gender_output_loss: 0.5385 - image_quality_output_loss: 0.9053 - age_output_loss: 1.4002 - weight_output_loss: 0.9739 - bag_output_loss: 0.8661 - footwear_output_loss: 0.8197 - pose_output_loss: 0.8977 - emotion_output_loss: 0.8939 - gender_output_acc: 0.7267 - image_quality_output_acc: 0.5658 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5975 - footwear_output_acc: 0.6317 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.0590 - val_gender_output_loss: 0.6183 - val_image_quality_output_loss: 1.0297 - val_age_output_loss: 1.4127 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.8884 - val_footwear_output_loss: 0.8566 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.9147 - val_gender_output_acc: 0.6449 - val_image_quality_output_acc: 0.5582 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5612 - val_footwear_output_acc: 0.6432 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 8.64272\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 92s 254ms/step - loss: 8.8222 - gender_output_loss: 0.5617 - image_quality_output_loss: 0.9165 - age_output_loss: 1.4032 - weight_output_loss: 0.9756 - bag_output_loss: 0.8781 - footwear_output_loss: 0.8416 - pose_output_loss: 0.9023 - emotion_output_loss: 0.8992 - gender_output_acc: 0.7121 - image_quality_output_acc: 0.5623 - age_output_acc: 0.3981 - weight_output_acc: 0.6344 - bag_output_acc: 0.5905 - footwear_output_acc: 0.6225 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 9.4070 - val_gender_output_loss: 0.7908 - val_image_quality_output_loss: 1.0169 - val_age_output_loss: 1.4575 - val_weight_output_loss: 0.9912 - val_bag_output_loss: 0.9289 - val_footwear_output_loss: 0.9338 - val_pose_output_loss: 0.9358 - val_emotion_output_loss: 0.9165 - val_gender_output_acc: 0.4834 - val_image_quality_output_acc: 0.5599 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.4881 - val_footwear_output_acc: 0.5835 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 8.64272\n",
            "Epoch 25/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.7648 - gender_output_loss: 0.5454 - image_quality_output_loss: 0.9116 - age_output_loss: 1.4051 - weight_output_loss: 0.9769 - bag_output_loss: 0.8736 - footwear_output_loss: 0.8283 - pose_output_loss: 0.8980 - emotion_output_loss: 0.8977 - gender_output_acc: 0.7239 - image_quality_output_acc: 0.5660 - age_output_acc: 0.3970 - weight_output_acc: 0.6344 - bag_output_acc: 0.5960 - footwear_output_acc: 0.6304 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.7553 - val_gender_output_loss: 0.5454 - val_image_quality_output_loss: 0.9193 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8860 - val_footwear_output_loss: 0.8074 - val_pose_output_loss: 0.8914 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.7155 - val_image_quality_output_acc: 0.5632 - val_age_output_acc: 0.3979 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5866 - val_footwear_output_acc: 0.6484 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 8.64272\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 8.6487 - gender_output_loss: 0.5053 - image_quality_output_loss: 0.9029 - age_output_loss: 1.4015 - weight_output_loss: 0.9754 - bag_output_loss: 0.8650 - footwear_output_loss: 0.8027 - pose_output_loss: 0.8847 - emotion_output_loss: 0.8928 - gender_output_acc: 0.7570 - image_quality_output_acc: 0.5715 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.6008 - footwear_output_acc: 0.6451 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 8.5630 - val_gender_output_loss: 0.4703 - val_image_quality_output_loss: 0.9162 - val_age_output_loss: 1.3960 - val_weight_output_loss: 0.9729 - val_bag_output_loss: 0.8585 - val_footwear_output_loss: 0.7630 - val_pose_output_loss: 0.8781 - val_emotion_output_loss: 0.8915 - val_gender_output_acc: 0.7922 - val_image_quality_output_acc: 0.5664 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6120 - val_footwear_output_acc: 0.6678 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00026: val_loss improved from 8.64272 to 8.56301, saving model to /content/gdrive/My Drive/model_at_epoch_26.hdf5\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 8.5948 - gender_output_loss: 0.4909 - image_quality_output_loss: 0.8959 - age_output_loss: 1.3988 - weight_output_loss: 0.9741 - bag_output_loss: 0.8643 - footwear_output_loss: 0.7901 - pose_output_loss: 0.8748 - emotion_output_loss: 0.8908 - gender_output_acc: 0.7648 - image_quality_output_acc: 0.5754 - age_output_acc: 0.3981 - weight_output_acc: 0.6344 - bag_output_acc: 0.6029 - footwear_output_acc: 0.6483 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7106 - val_loss: 8.5719 - val_gender_output_loss: 0.4960 - val_image_quality_output_loss: 0.8805 - val_age_output_loss: 1.4042 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.8651 - val_footwear_output_loss: 0.7783 - val_pose_output_loss: 0.8653 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7534 - val_image_quality_output_acc: 0.5733 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6016 - val_footwear_output_acc: 0.6579 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 8.56301\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 8.6893 - gender_output_loss: 0.5235 - image_quality_output_loss: 0.9099 - age_output_loss: 1.4028 - weight_output_loss: 0.9757 - bag_output_loss: 0.8721 - footwear_output_loss: 0.8193 - pose_output_loss: 0.8832 - emotion_output_loss: 0.8956 - gender_output_acc: 0.7456 - image_quality_output_acc: 0.5630 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.5944 - footwear_output_acc: 0.6364 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7106 - val_loss: 9.0838 - val_gender_output_loss: 0.5607 - val_image_quality_output_loss: 1.0861 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9785 - val_bag_output_loss: 0.8789 - val_footwear_output_loss: 0.9556 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.7180 - val_image_quality_output_acc: 0.5515 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.5660 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 8.56301\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.6936 - gender_output_loss: 0.5349 - image_quality_output_loss: 0.9160 - age_output_loss: 1.4038 - weight_output_loss: 0.9767 - bag_output_loss: 0.8737 - footwear_output_loss: 0.8205 - pose_output_loss: 0.8797 - emotion_output_loss: 0.8960 - gender_output_acc: 0.7335 - image_quality_output_acc: 0.5639 - age_output_acc: 0.3959 - weight_output_acc: 0.6344 - bag_output_acc: 0.5979 - footwear_output_acc: 0.6380 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7106 - val_loss: 8.7120 - val_gender_output_loss: 0.5214 - val_image_quality_output_loss: 0.9221 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8749 - val_footwear_output_loss: 0.8035 - val_pose_output_loss: 0.9212 - val_emotion_output_loss: 0.8984 - val_gender_output_acc: 0.7513 - val_image_quality_output_acc: 0.5618 - val_age_output_acc: 0.3975 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5999 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.6167 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 8.56301\n",
            "Epoch 30/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 8.5505 - gender_output_loss: 0.4852 - image_quality_output_loss: 0.9062 - age_output_loss: 1.4035 - weight_output_loss: 0.9747 - bag_output_loss: 0.8644 - footwear_output_loss: 0.7981 - pose_output_loss: 0.8455 - emotion_output_loss: 0.8925 - gender_output_acc: 0.7695 - image_quality_output_acc: 0.5665 - age_output_acc: 0.3976 - weight_output_acc: 0.6344 - bag_output_acc: 0.6037 - footwear_output_acc: 0.6458 - pose_output_acc: 0.6258 - emotion_output_acc: 0.7106 - val_loss: 8.6254 - val_gender_output_loss: 0.4833 - val_image_quality_output_loss: 0.9896 - val_age_output_loss: 1.4026 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8707 - val_footwear_output_loss: 0.7734 - val_pose_output_loss: 0.8571 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.7786 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5995 - val_footwear_output_acc: 0.6664 - val_pose_output_acc: 0.6210 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 8.56301\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 8.4300 - gender_output_loss: 0.4520 - image_quality_output_loss: 0.8918 - age_output_loss: 1.3959 - weight_output_loss: 0.9730 - bag_output_loss: 0.8550 - footwear_output_loss: 0.7807 - pose_output_loss: 0.8186 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7889 - image_quality_output_acc: 0.5729 - age_output_acc: 0.3977 - weight_output_acc: 0.6344 - bag_output_acc: 0.6137 - footwear_output_acc: 0.6624 - pose_output_acc: 0.6398 - emotion_output_acc: 0.7106 - val_loss: 8.5807 - val_gender_output_loss: 0.5204 - val_image_quality_output_loss: 0.9485 - val_age_output_loss: 1.4067 - val_weight_output_loss: 0.9790 - val_bag_output_loss: 0.8698 - val_footwear_output_loss: 0.7746 - val_pose_output_loss: 0.8149 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.7459 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5933 - val_footwear_output_acc: 0.6629 - val_pose_output_acc: 0.6245 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 8.56301\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.5093 - gender_output_loss: 0.4855 - image_quality_output_loss: 0.9053 - age_output_loss: 1.4007 - weight_output_loss: 0.9760 - bag_output_loss: 0.8609 - footwear_output_loss: 0.7964 - pose_output_loss: 0.8235 - emotion_output_loss: 0.8903 - gender_output_acc: 0.7719 - image_quality_output_acc: 0.5668 - age_output_acc: 0.3985 - weight_output_acc: 0.6344 - bag_output_acc: 0.6017 - footwear_output_acc: 0.6479 - pose_output_acc: 0.6322 - emotion_output_acc: 0.7106 - val_loss: 9.0116 - val_gender_output_loss: 0.6956 - val_image_quality_output_loss: 0.9762 - val_age_output_loss: 1.4221 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.9133 - val_footwear_output_loss: 0.8901 - val_pose_output_loss: 0.8481 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.5975 - val_image_quality_output_acc: 0.5324 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5160 - val_footwear_output_acc: 0.5786 - val_pose_output_acc: 0.6221 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 8.56301\n",
            "Epoch 33/50\n",
            "360/360 [==============================] - 90s 250ms/step - loss: 8.5820 - gender_output_loss: 0.5114 - image_quality_output_loss: 0.9176 - age_output_loss: 1.4047 - weight_output_loss: 0.9763 - bag_output_loss: 0.8713 - footwear_output_loss: 0.8164 - pose_output_loss: 0.8319 - emotion_output_loss: 0.8938 - gender_output_acc: 0.7536 - image_quality_output_acc: 0.5627 - age_output_acc: 0.3984 - weight_output_acc: 0.6344 - bag_output_acc: 0.5977 - footwear_output_acc: 0.6392 - pose_output_acc: 0.6313 - emotion_output_acc: 0.7106 - val_loss: 8.7962 - val_gender_output_loss: 0.5803 - val_image_quality_output_loss: 0.9586 - val_age_output_loss: 1.4075 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.8842 - val_footwear_output_loss: 0.8346 - val_pose_output_loss: 0.8988 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.7105 - val_image_quality_output_acc: 0.5563 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5694 - val_footwear_output_acc: 0.6346 - val_pose_output_acc: 0.6177 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 8.56301\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 8.4525 - gender_output_loss: 0.4866 - image_quality_output_loss: 0.9068 - age_output_loss: 1.4009 - weight_output_loss: 0.9765 - bag_output_loss: 0.8639 - footwear_output_loss: 0.8004 - pose_output_loss: 0.7838 - emotion_output_loss: 0.8881 - gender_output_acc: 0.7690 - image_quality_output_acc: 0.5649 - age_output_acc: 0.3984 - weight_output_acc: 0.6344 - bag_output_acc: 0.6039 - footwear_output_acc: 0.6493 - pose_output_acc: 0.6515 - emotion_output_acc: 0.7106 - val_loss: 8.2633 - val_gender_output_loss: 0.4269 - val_image_quality_output_loss: 0.8825 - val_age_output_loss: 1.3954 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.8595 - val_footwear_output_loss: 0.7582 - val_pose_output_loss: 0.7441 - val_emotion_output_loss: 0.8800 - val_gender_output_acc: 0.8073 - val_image_quality_output_acc: 0.5681 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6069 - val_footwear_output_acc: 0.6745 - val_pose_output_acc: 0.6793 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00034: val_loss improved from 8.56301 to 8.26328, saving model to /content/gdrive/My Drive/model_at_epoch_34.hdf5\n",
            "Epoch 35/50\n",
            "360/360 [==============================] - 90s 251ms/step - loss: 8.2906 - gender_output_loss: 0.4390 - image_quality_output_loss: 0.8974 - age_output_loss: 1.3951 - weight_output_loss: 0.9745 - bag_output_loss: 0.8531 - footwear_output_loss: 0.7730 - pose_output_loss: 0.7388 - emotion_output_loss: 0.8804 - gender_output_acc: 0.8006 - image_quality_output_acc: 0.5766 - age_output_acc: 0.3981 - weight_output_acc: 0.6344 - bag_output_acc: 0.6155 - footwear_output_acc: 0.6661 - pose_output_acc: 0.6740 - emotion_output_acc: 0.7106 - val_loss: 8.1446 - val_gender_output_loss: 0.3915 - val_image_quality_output_loss: 0.8970 - val_age_output_loss: 1.3934 - val_weight_output_loss: 0.9747 - val_bag_output_loss: 0.8447 - val_footwear_output_loss: 0.7343 - val_pose_output_loss: 0.6940 - val_emotion_output_loss: 0.8767 - val_gender_output_acc: 0.8372 - val_image_quality_output_acc: 0.5697 - val_age_output_acc: 0.3980 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6196 - val_footwear_output_acc: 0.6817 - val_pose_output_acc: 0.6789 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00035: val_loss improved from 8.26328 to 8.14459, saving model to /content/gdrive/My Drive/model_at_epoch_35.hdf5\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.3044 - gender_output_loss: 0.4471 - image_quality_output_loss: 0.9032 - age_output_loss: 1.3967 - weight_output_loss: 0.9749 - bag_output_loss: 0.8522 - footwear_output_loss: 0.7743 - pose_output_loss: 0.7401 - emotion_output_loss: 0.8799 - gender_output_acc: 0.7936 - image_quality_output_acc: 0.5708 - age_output_acc: 0.3990 - weight_output_acc: 0.6344 - bag_output_acc: 0.6142 - footwear_output_acc: 0.6635 - pose_output_acc: 0.6671 - emotion_output_acc: 0.7106 - val_loss: 8.6271 - val_gender_output_loss: 0.6473 - val_image_quality_output_loss: 0.8972 - val_age_output_loss: 1.4171 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.9016 - val_footwear_output_loss: 0.8109 - val_pose_output_loss: 0.7531 - val_emotion_output_loss: 0.8858 - val_gender_output_acc: 0.7091 - val_image_quality_output_acc: 0.5668 - val_age_output_acc: 0.3960 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5908 - val_footwear_output_acc: 0.6416 - val_pose_output_acc: 0.6701 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 8.14459\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 8.4300 - gender_output_loss: 0.4964 - image_quality_output_loss: 0.9106 - age_output_loss: 1.3999 - weight_output_loss: 0.9755 - bag_output_loss: 0.8619 - footwear_output_loss: 0.8049 - pose_output_loss: 0.7686 - emotion_output_loss: 0.8856 - gender_output_acc: 0.7640 - image_quality_output_acc: 0.5639 - age_output_acc: 0.3980 - weight_output_acc: 0.6344 - bag_output_acc: 0.6046 - footwear_output_acc: 0.6475 - pose_output_acc: 0.6613 - emotion_output_acc: 0.7106 - val_loss: 8.5881 - val_gender_output_loss: 0.5784 - val_image_quality_output_loss: 0.9125 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9806 - val_bag_output_loss: 0.9033 - val_footwear_output_loss: 0.7957 - val_pose_output_loss: 0.7940 - val_emotion_output_loss: 0.8905 - val_gender_output_acc: 0.7212 - val_image_quality_output_acc: 0.5603 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5893 - val_footwear_output_acc: 0.6637 - val_pose_output_acc: 0.6422 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 8.14459\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 91s 253ms/step - loss: 8.3797 - gender_output_loss: 0.4843 - image_quality_output_loss: 0.9091 - age_output_loss: 1.3984 - weight_output_loss: 0.9769 - bag_output_loss: 0.8615 - footwear_output_loss: 0.8024 - pose_output_loss: 0.7516 - emotion_output_loss: 0.8820 - gender_output_acc: 0.7697 - image_quality_output_acc: 0.5622 - age_output_acc: 0.3987 - weight_output_acc: 0.6344 - bag_output_acc: 0.6083 - footwear_output_acc: 0.6485 - pose_output_acc: 0.6669 - emotion_output_acc: 0.7106 - val_loss: 8.2283 - val_gender_output_loss: 0.4333 - val_image_quality_output_loss: 0.9372 - val_age_output_loss: 1.3992 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8661 - val_footwear_output_loss: 0.7492 - val_pose_output_loss: 0.6834 - val_emotion_output_loss: 0.8750 - val_gender_output_acc: 0.8012 - val_image_quality_output_acc: 0.5667 - val_age_output_acc: 0.3974 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6036 - val_footwear_output_acc: 0.6808 - val_pose_output_acc: 0.6915 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 8.14459\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 91s 252ms/step - loss: 8.1969 - gender_output_loss: 0.4294 - image_quality_output_loss: 0.9003 - age_output_loss: 1.3958 - weight_output_loss: 0.9755 - bag_output_loss: 0.8540 - footwear_output_loss: 0.7632 - pose_output_loss: 0.6981 - emotion_output_loss: 0.8755 - gender_output_acc: 0.8058 - image_quality_output_acc: 0.5746 - age_output_acc: 0.3984 - weight_output_acc: 0.6344 - bag_output_acc: 0.6143 - footwear_output_acc: 0.6707 - pose_output_acc: 0.6935 - emotion_output_acc: 0.7106 - val_loss: 7.9799 - val_gender_output_loss: 0.3831 - val_image_quality_output_loss: 0.8716 - val_age_output_loss: 1.3905 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8416 - val_footwear_output_loss: 0.7093 - val_pose_output_loss: 0.6369 - val_emotion_output_loss: 0.8688 - val_gender_output_acc: 0.8396 - val_image_quality_output_acc: 0.5769 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6271 - val_footwear_output_acc: 0.6937 - val_pose_output_acc: 0.7215 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00039: val_loss improved from 8.14459 to 7.97995, saving model to /content/gdrive/My Drive/model_at_epoch_39.hdf5\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 8.1165 - gender_output_loss: 0.4062 - image_quality_output_loss: 0.8957 - age_output_loss: 1.3922 - weight_output_loss: 0.9749 - bag_output_loss: 0.8437 - footwear_output_loss: 0.7504 - pose_output_loss: 0.6799 - emotion_output_loss: 0.8713 - gender_output_acc: 0.8194 - image_quality_output_acc: 0.5753 - age_output_acc: 0.3984 - weight_output_acc: 0.6344 - bag_output_acc: 0.6257 - footwear_output_acc: 0.6735 - pose_output_acc: 0.6953 - emotion_output_acc: 0.7106 - val_loss: 8.4165 - val_gender_output_loss: 0.4732 - val_image_quality_output_loss: 0.9248 - val_age_output_loss: 1.4030 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8714 - val_footwear_output_loss: 0.7845 - val_pose_output_loss: 0.7989 - val_emotion_output_loss: 0.8820 - val_gender_output_acc: 0.7711 - val_image_quality_output_acc: 0.5604 - val_age_output_acc: 0.3995 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5806 - val_footwear_output_acc: 0.6556 - val_pose_output_acc: 0.6639 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.97995\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 92s 254ms/step - loss: 8.2637 - gender_output_loss: 0.4536 - image_quality_output_loss: 0.9094 - age_output_loss: 1.3961 - weight_output_loss: 0.9768 - bag_output_loss: 0.8566 - footwear_output_loss: 0.7783 - pose_output_loss: 0.7195 - emotion_output_loss: 0.8777 - gender_output_acc: 0.7890 - image_quality_output_acc: 0.5701 - age_output_acc: 0.4003 - weight_output_acc: 0.6344 - bag_output_acc: 0.6143 - footwear_output_acc: 0.6635 - pose_output_acc: 0.6813 - emotion_output_acc: 0.7106 - val_loss: 8.4717 - val_gender_output_loss: 0.4848 - val_image_quality_output_loss: 0.9613 - val_age_output_loss: 1.4009 - val_weight_output_loss: 0.9763 - val_bag_output_loss: 0.8660 - val_footwear_output_loss: 0.8959 - val_pose_output_loss: 0.7085 - val_emotion_output_loss: 0.8876 - val_gender_output_acc: 0.7773 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6077 - val_footwear_output_acc: 0.5881 - val_pose_output_acc: 0.6803 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 7.97995\n",
            "Epoch 42/50\n",
            "360/360 [==============================] - 92s 254ms/step - loss: 8.3183 - gender_output_loss: 0.4755 - image_quality_output_loss: 0.9139 - age_output_loss: 1.3993 - weight_output_loss: 0.9761 - bag_output_loss: 0.8628 - footwear_output_loss: 0.7926 - pose_output_loss: 0.7326 - emotion_output_loss: 0.8814 - gender_output_acc: 0.7791 - image_quality_output_acc: 0.5661 - age_output_acc: 0.3988 - weight_output_acc: 0.6344 - bag_output_acc: 0.6083 - footwear_output_acc: 0.6546 - pose_output_acc: 0.6764 - emotion_output_acc: 0.7106 - val_loss: 8.3882 - val_gender_output_loss: 0.6166 - val_image_quality_output_loss: 0.8972 - val_age_output_loss: 1.4068 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.8913 - val_footwear_output_loss: 0.7978 - val_pose_output_loss: 0.6483 - val_emotion_output_loss: 0.8733 - val_gender_output_acc: 0.6868 - val_image_quality_output_acc: 0.5647 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5890 - val_footwear_output_acc: 0.6506 - val_pose_output_acc: 0.7173 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.97995\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 91s 254ms/step - loss: 8.1317 - gender_output_loss: 0.4217 - image_quality_output_loss: 0.8989 - age_output_loss: 1.3936 - weight_output_loss: 0.9760 - bag_output_loss: 0.8473 - footwear_output_loss: 0.7621 - pose_output_loss: 0.6857 - emotion_output_loss: 0.8724 - gender_output_acc: 0.8102 - image_quality_output_acc: 0.5747 - age_output_acc: 0.4027 - weight_output_acc: 0.6344 - bag_output_acc: 0.6177 - footwear_output_acc: 0.6721 - pose_output_acc: 0.6982 - emotion_output_acc: 0.7106 - val_loss: 7.9415 - val_gender_output_loss: 0.3635 - val_image_quality_output_loss: 0.8998 - val_age_output_loss: 1.3900 - val_weight_output_loss: 0.9754 - val_bag_output_loss: 0.8375 - val_footwear_output_loss: 0.7179 - val_pose_output_loss: 0.6209 - val_emotion_output_loss: 0.8655 - val_gender_output_acc: 0.8468 - val_image_quality_output_acc: 0.5678 - val_age_output_acc: 0.3986 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6289 - val_footwear_output_acc: 0.6909 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00043: val_loss improved from 7.97995 to 7.94145, saving model to /content/gdrive/My Drive/model_at_epoch_43.hdf5\n",
            "Epoch 44/50\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 7.9792 - gender_output_loss: 0.3727 - image_quality_output_loss: 0.8909 - age_output_loss: 1.3905 - weight_output_loss: 0.9738 - bag_output_loss: 0.8410 - footwear_output_loss: 0.7320 - pose_output_loss: 0.6417 - emotion_output_loss: 0.8667 - gender_output_acc: 0.8433 - image_quality_output_acc: 0.5806 - age_output_acc: 0.4023 - weight_output_acc: 0.6344 - bag_output_acc: 0.6254 - footwear_output_acc: 0.6840 - pose_output_acc: 0.7209 - emotion_output_acc: 0.7106 - val_loss: 7.8922 - val_gender_output_loss: 0.3587 - val_image_quality_output_loss: 0.8706 - val_age_output_loss: 1.3923 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8389 - val_footwear_output_loss: 0.7146 - val_pose_output_loss: 0.6114 - val_emotion_output_loss: 0.8617 - val_gender_output_acc: 0.8450 - val_image_quality_output_acc: 0.5766 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6298 - val_footwear_output_acc: 0.6857 - val_pose_output_acc: 0.7321 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00044: val_loss improved from 7.94145 to 7.89222, saving model to /content/gdrive/My Drive/model_at_epoch_44.hdf5\n",
            "Epoch 45/50\n",
            "360/360 [==============================] - 92s 255ms/step - loss: 8.0791 - gender_output_loss: 0.4093 - image_quality_output_loss: 0.8986 - age_output_loss: 1.3926 - weight_output_loss: 0.9752 - bag_output_loss: 0.8455 - footwear_output_loss: 0.7491 - pose_output_loss: 0.6746 - emotion_output_loss: 0.8684 - gender_output_acc: 0.8201 - image_quality_output_acc: 0.5761 - age_output_acc: 0.4009 - weight_output_acc: 0.6344 - bag_output_acc: 0.6234 - footwear_output_acc: 0.6748 - pose_output_acc: 0.7042 - emotion_output_acc: 0.7106 - val_loss: 9.2834 - val_gender_output_loss: 1.0069 - val_image_quality_output_loss: 0.9568 - val_age_output_loss: 1.4315 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9300 - val_footwear_output_loss: 0.8713 - val_pose_output_loss: 0.9359 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.6416 - val_image_quality_output_acc: 0.5609 - val_age_output_acc: 0.3825 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5847 - val_footwear_output_acc: 0.6130 - val_pose_output_acc: 0.6661 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 7.89222\n",
            "Epoch 46/50\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 8.2313 - gender_output_loss: 0.4649 - image_quality_output_loss: 0.9122 - age_output_loss: 1.3966 - weight_output_loss: 0.9761 - bag_output_loss: 0.8547 - footwear_output_loss: 0.7879 - pose_output_loss: 0.7054 - emotion_output_loss: 0.8771 - gender_output_acc: 0.7851 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4013 - weight_output_acc: 0.6344 - bag_output_acc: 0.6140 - footwear_output_acc: 0.6581 - pose_output_acc: 0.6929 - emotion_output_acc: 0.7106 - val_loss: 8.4785 - val_gender_output_loss: 0.6972 - val_image_quality_output_loss: 0.9021 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9809 - val_bag_output_loss: 0.9169 - val_footwear_output_loss: 0.7712 - val_pose_output_loss: 0.6653 - val_emotion_output_loss: 0.8778 - val_gender_output_acc: 0.6587 - val_image_quality_output_acc: 0.5661 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5839 - val_footwear_output_acc: 0.6698 - val_pose_output_acc: 0.7103 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 7.89222\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 92s 257ms/step - loss: 8.0881 - gender_output_loss: 0.4215 - image_quality_output_loss: 0.9047 - age_output_loss: 1.3930 - weight_output_loss: 0.9750 - bag_output_loss: 0.8467 - footwear_output_loss: 0.7541 - pose_output_loss: 0.6736 - emotion_output_loss: 0.8738 - gender_output_acc: 0.8119 - image_quality_output_acc: 0.5741 - age_output_acc: 0.3983 - weight_output_acc: 0.6344 - bag_output_acc: 0.6231 - footwear_output_acc: 0.6734 - pose_output_acc: 0.7068 - emotion_output_acc: 0.7106 - val_loss: 8.1270 - val_gender_output_loss: 0.5476 - val_image_quality_output_loss: 0.9177 - val_age_output_loss: 1.3917 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8634 - val_footwear_output_loss: 0.7260 - val_pose_output_loss: 0.5955 - val_emotion_output_loss: 0.8679 - val_gender_output_acc: 0.7424 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.3976 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6116 - val_footwear_output_acc: 0.6879 - val_pose_output_acc: 0.7438 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 7.89222\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 92s 256ms/step - loss: 7.8843 - gender_output_loss: 0.3588 - image_quality_output_loss: 0.8834 - age_output_loss: 1.3860 - weight_output_loss: 0.9739 - bag_output_loss: 0.8353 - footwear_output_loss: 0.7206 - pose_output_loss: 0.6206 - emotion_output_loss: 0.8656 - gender_output_acc: 0.8473 - image_quality_output_acc: 0.5829 - age_output_acc: 0.4003 - weight_output_acc: 0.6344 - bag_output_acc: 0.6353 - footwear_output_acc: 0.6898 - pose_output_acc: 0.7336 - emotion_output_acc: 0.7106 - val_loss: 7.6273 - val_gender_output_loss: 0.2935 - val_image_quality_output_loss: 0.8484 - val_age_output_loss: 1.3819 - val_weight_output_loss: 0.9734 - val_bag_output_loss: 0.8212 - val_footwear_output_loss: 0.6744 - val_pose_output_loss: 0.5399 - val_emotion_output_loss: 0.8556 - val_gender_output_acc: 0.8884 - val_image_quality_output_acc: 0.5878 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6445 - val_footwear_output_acc: 0.7111 - val_pose_output_acc: 0.7783 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00048: val_loss improved from 7.89222 to 7.62734, saving model to /content/gdrive/My Drive/model_at_epoch_48.hdf5\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 92s 257ms/step - loss: 7.9031 - gender_output_loss: 0.3664 - image_quality_output_loss: 0.8868 - age_output_loss: 1.3885 - weight_output_loss: 0.9748 - bag_output_loss: 0.8349 - footwear_output_loss: 0.7211 - pose_output_loss: 0.6263 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8407 - image_quality_output_acc: 0.5795 - age_output_acc: 0.4003 - weight_output_acc: 0.6344 - bag_output_acc: 0.6336 - footwear_output_acc: 0.6919 - pose_output_acc: 0.7312 - emotion_output_acc: 0.7106 - val_loss: 8.8113 - val_gender_output_loss: 0.9332 - val_image_quality_output_loss: 0.9661 - val_age_output_loss: 1.4221 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9282 - val_footwear_output_loss: 0.7670 - val_pose_output_loss: 0.6917 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.6433 - val_image_quality_output_acc: 0.5649 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.5845 - val_footwear_output_acc: 0.6806 - val_pose_output_acc: 0.7151 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 7.62734\n",
            "Epoch 50/50\n",
            "360/360 [==============================] - 93s 258ms/step - loss: 8.0934 - gender_output_loss: 0.4240 - image_quality_output_loss: 0.9101 - age_output_loss: 1.3904 - weight_output_loss: 0.9745 - bag_output_loss: 0.8510 - footwear_output_loss: 0.7686 - pose_output_loss: 0.6720 - emotion_output_loss: 0.8726 - gender_output_acc: 0.8120 - image_quality_output_acc: 0.5681 - age_output_acc: 0.4001 - weight_output_acc: 0.6344 - bag_output_acc: 0.6195 - footwear_output_acc: 0.6687 - pose_output_acc: 0.7069 - emotion_output_acc: 0.7106 - val_loss: 8.3734 - val_gender_output_loss: 0.4367 - val_image_quality_output_loss: 1.0127 - val_age_output_loss: 1.4019 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.8687 - val_footwear_output_loss: 0.8051 - val_pose_output_loss: 0.7633 - val_emotion_output_loss: 0.8801 - val_gender_output_acc: 0.8007 - val_image_quality_output_acc: 0.5379 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6342 - val_bag_output_acc: 0.6081 - val_footwear_output_acc: 0.6516 - val_pose_output_acc: 0.6488 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 7.62734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb64dee2da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBPDhJ27sQyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "d08a5fcb-c3c9-4459-f941-9737209b1e0e"
      },
      "source": [
        "def evaluate_model(model):\n",
        "    results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "    accuracies = {}\n",
        "    losses = {}\n",
        "    for k, v in zip(model.metrics_names, results):\n",
        "        if k.endswith('_acc'):\n",
        "            accuracies[k] = round(v * 100, 4) \n",
        "        else:\n",
        "            losses[k] = v\n",
        "    return accuracies\n",
        "    \n",
        "evaluate_model(model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 16s 89ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 39.2622,\n",
              " 'bag_output_acc': 60.8073,\n",
              " 'emotion_output_acc': 71.0764,\n",
              " 'footwear_output_acc': 65.1649,\n",
              " 'gender_output_acc': 80.0694,\n",
              " 'image_quality_output_acc': 53.7934,\n",
              " 'pose_output_acc': 64.8785,\n",
              " 'weight_output_acc': 63.4201}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}